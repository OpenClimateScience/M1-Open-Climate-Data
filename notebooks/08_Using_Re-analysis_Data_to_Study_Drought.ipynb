{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79bde19-ec9b-401e-8ad0-6bfad1ad6dc3",
   "metadata": {},
   "source": [
    "# M1.8 - Using Re-Analysis Data to Study Drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24afa7-54b1-45bc-ab6b-9164cab19231",
   "metadata": {},
   "source": [
    "*Part of:* **M1: Open Climate Data**\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "1. [Organizing our file system](#Organizing-our-file-system)\n",
    "2. [Downloading NLDAS data](#Downloading-NLDAS-data)\n",
    "3. [Understanding the netCDF file format](#Understanding-the-netCDF-file-format)\n",
    "   - [Getting real values from netCDF datasets](#Getting-real-values-from-netCDF-datasets)\n",
    "   - [Plotting netCDF4 variables](#Plotting-netCDF4-variables)\n",
    "4. [Opening multiple files with `xarray`](#Opening-multiple-files-with-xarray)\n",
    "5. [Computing a climatology](#Computing-a-climatology)\n",
    "   - [Describing climatic extremes using anomalies](#Describing-climatic-extremes-using-anomalies)\n",
    "6. [Writing re-useable code and documentation](#Writing-re-useable-code-and-documentation)\n",
    "   - [Repeating our climate data analysis](#Repeating-our-climate-data-analysis)\n",
    "7. [Computing vapor pressure deficit](#Computing-vapor-pressure-deficit)\n",
    "   - [Saving our results](#Saving-our-results)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea7189-42c4-4f69-89c1-99053040b61b",
   "metadata": {},
   "source": [
    "Let's use everything we've learned so far to study a real drought event in an agricultural system.\n",
    "\n",
    "Whereas meteorological drought is characterized by a deficit of precipitation, **flash drought** is characterized by a sudden, extreme demand for water from the land surface: \"anomalously high evapotranspiration rates, caused by anomalously high temperatures, winds, and/or incoming radiation\" (Chen et al. 2019).\n",
    "\n",
    "In 2017, a flash drought emerged in the Northern Plains of the United States. The U.S. Drought Monitor estimated that, on September 5, 2017, about 23% of the region experienced \"extreme drought\" conditions and subsequent crop losses and water shortages (He et al. 2019, *Environmental Research Letters*).\n",
    "\n",
    "**For this case study, we're going to consider the following climate variables and data sources:**\n",
    "\n",
    "- Evapotranspiration, radiation, and soil moisture data from [the North American Land Data Assimilation System (NLDAS)](https://disc.gsfc.nasa.gov/datasets/NLDAS_NOAH0125_M_2.0/summary?keywords=NLDAS), a re-analysis dataset.\n",
    "- Air temperature, pressure, and humidity, from [the NLDAS forcing data](https://disc.gsfc.nasa.gov/datasets/NLDAS_FORA0125_M_2.0/summary?keywords=NLDAS)\n",
    "- Soil moisture the Soil Moisture Active Passive (SMAP) mission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c13a88-9869-4dad-8384-e863e384589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7b524-c0a6-46d7-9a8c-255d2a9a5c55",
   "metadata": {},
   "source": [
    "## Organizing our file system\n",
    "\n",
    "We'll need a place to store these raw data. It's important that we have a folder in our file system reserved for these raw data so we can keep them separate from any new datasets (outputs) we might create. \n",
    "\n",
    "**Create a folder called `outputs` in your Jupyter Notebook's file system.** You can do this from the Jupyter Notebook home page (the file tree) by selecting \"New\" and then \"New Folder\" as in the screenshot below.\n",
    "\n",
    "![](./assets/M1_screenshot_Jupyter_new_folder.png)\n",
    "\n",
    "**Let's also create a folder called `data_raw` in our Jupyter Notebook's file system. Create two sub-folders within `data_raw`:**\n",
    "\n",
    "- `NLDAS`\n",
    "- `SMAP_L3`\n",
    "\n",
    "We should never modify the raw data (that we're about to download). Doing so would make it hard to repeat the analysis we're going to perform as we will lose the original data values. This doesn't mean we have to keep the `data_raw` folder around forever: if it's publicly available data, we can always download it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22575e4c-dada-4ebe-8d8e-67b5f55fc39d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Downloading NLDAS data\n",
    "\n",
    "We'll use the function `earthaccess.search_data()` again. In this case, the `short_name` and `version` can be found on [the Goddard Earth Sciences (GES) Data and Information Services Center (DISC) website for this product.](https://disc.gsfc.nasa.gov/datasets/NLDAS_NOAH0125_M_2.0/summary?keywords=NLDAS)\n",
    "\n",
    "**The NLDAS data we're interested in are compiled monthly and we want to download an August monthly dataset for each year.** Because the dates we want are non-consecutive, we need to call `earthaccess.search_data()` within a `for` loop. Below, we also use string formatting so that the string `f'{year}-08'` becomes, e.g.: `'2008-08'`, `'2009-08'`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cde0a-21a3-4a6d-bc78-78aa58b51804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor: Show how to find the \"short_name\" and \"version\"\n",
    "\n",
    "results = []\n",
    "\n",
    "# Get data from August for every year from 2008 up to (but not including) 2018\n",
    "for year in range(2008, 2018):\n",
    "    search = earthaccess.search_data(\n",
    "        short_name = 'NLDAS_NOAH0125_M',\n",
    "        version = '2.0',\n",
    "        temporal = (f'{year}-08', f'{year}-08'))\n",
    "    results.extend(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f8847-6347-4ad7-8179-de355a4ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12eab9d-a5d7-4eef-8191-140c30c037e8",
   "metadata": {},
   "source": [
    "Previously, we've used `earthaccess.open()` to get access to these data. This time, we'll use `earthaccess.download()`. What's the difference?\n",
    "\n",
    "- `earthaccess.open()` provides a file-like object that is available to be downloaded and read *only we need it.*\n",
    "- `earthaccess.download()` actually downloads the file to our file system.\n",
    "\n",
    "**Note that, below, we're telling `earthaccess.download()` to put the downloaded files into our new `data_raw` folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3692c6-ae6c-4f6d-b919-747917eb9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(results, 'data_raw/NLDAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcd7b2-9b50-4367-847e-dce4cae46fdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding the netCDF file format\n",
    "\n",
    "Although we could open these netCDF files using `xarray`, we're going to first use a different Python library called `netCDF4`. This will help us learn more about the netCDF file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8743148-9ae6-434e-89bc-ee10f2e3296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "\n",
    "nc = netCDF4.Dataset('data_raw/NLDAS/NLDAS_NOAH0125_M.A200808.020.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0479a8-8cc0-4dee-bcf1-5003badaa8f1",
   "metadata": {},
   "source": [
    "As we previously discussed, one of the advantages of the netCDF file format is that it is **self-documenting;** there are file-level and dataset-level descriptive information, or metadata, called **attributes.**\n",
    "\n",
    "When using the `netCDF4` library to open a netCDF file, we can access **file-level attributes** this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47921a58-b40f-4fc9-8f57-99187054d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.ncattrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9202eb-b4f3-4772-a560-6ac83006ac7b",
   "metadata": {},
   "source": [
    "And we can access **dataset-level attributes** this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375fdb-7939-48c0-9b3c-76dde699858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc['Evap'].ncattrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf959b-7041-4f53-b031-f40ea60eb250",
   "metadata": {},
   "source": [
    "### Getting real values from netCDF datasets\n",
    "\n",
    "There are some significant differences between how the `xarray` and `netCDF4` libraries represent netCDF files. One important difference is how array data are read from the files.\n",
    "\n",
    "The `netCDF4` package reads array data from the file the way it is stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a08166-9af5-4359-a061-45df888bc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc['Evap']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66a146-d0ab-4674-b24f-2539a81f0a10",
   "metadata": {},
   "source": [
    "**Notice the `scale_factor`, `add_offset`, and `missing_value` attributes.** These are very important to consider because of the way netCDF4 files sometimes store variables. If the variables are packed in a certain way to save disk space, we need to transform the packed values into real values before using the data:\n",
    "\n",
    "$$\n",
    "\\text{Real value} = (\\text{Packed value}\\times \\text{Scale factor}) + \\text{Offset}\n",
    "$$\n",
    "\n",
    "**When we look at the `\"Evap\"` (total evapotranspiration) dataset's attributes with `xarray`, however...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfff4f-9a2a-486a-a89d-44a477612ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data_raw/NLDAS/NLDAS_NOAH0125_M.A200808.020.nc')\n",
    "\n",
    "ds['Evap'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb860609-7556-4dfd-868c-c353a791f81d",
   "metadata": {},
   "source": [
    "**Note that the attributes are different!** The `scale_factor`, `add_offset`, and `missing_value` attributes are missing.\n",
    "\n",
    "**This is because `xarray` transforms packed values into real values automatically for us.** Compare the two examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c1fa0-176f-4380-9209-ef5d3cbee4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(nc['Evap'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4e8c7-4d0b-4f3a-85d5-ff44bb342999",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ds['Evap'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0c869-d4ad-4c76-b7a6-cd0ad40e36e3",
   "metadata": {},
   "source": [
    "**In this case, the `scale_factor` is `1.0` and the `add_offset` is `0.0`, meaning the packed values are the same as the real values.** Hence, there is no difference in the numbers for valid data areas, above; but we can see that the `xarray.Dataset` replaced the `missing_value` (-9999) with `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3eea39-9872-42ce-be6e-958c471bee12",
   "metadata": {},
   "source": [
    "### Plotting netCDF4 variables\n",
    "\n",
    "Let's plot the data from the first monthly dataset (August 2008). **Recall that, when using `pyplot.imshow()`, we have to provide a 2D array...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a17ca7-9a8c-4cb6-8068-55338d46db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "et = nc['Evap']\n",
    "\n",
    "et.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8d8b5-7d87-4464-93b4-69ce51a1f90b",
   "metadata": {},
   "source": [
    "The first axis of our `et` array is trivial, as it has only one element. We can simply subset the `et` array to this \"first\" (and only) element using `et[0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0e2bd-12db-432f-af8c-f684d98b73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(et[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f917ad-80e6-4c06-9fd1-904cd3bce7ee",
   "metadata": {},
   "source": [
    "Does this look right? Why is it upside down?\n",
    "\n",
    "The reason is because of [the CF Convention](http://cfconventions.org/) that defines how netCDF4 files should be formatted. Part of that standard requires that the coordinate arrays (here, latitude and longitude arrays) be sorted from smallest number to largest number. **Whereas spatial coordinate systems like latitude-longitude have numbers increasing from bottom-to-top and left-to-right, image coordinate systems (for arrays) differ in that numbers increase from top-to-bottom:**\n",
    "\n",
    "![](assets/coordinate-system-diagram.png)\n",
    "\n",
    "When working with a coordinate system that uses latitude, that means that the vertical coordinates go from the most southern (negative) latitude to the most northern (positive) latitude. **Essentially, are image is flipped upside-down because the most negative coordinates are at the top of the image.** We can easily flip an array right-side up using `np.flipud()` (\"flip upside-down\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dd59c-b288-41c3-818c-2039cda5c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(np.flipud(et[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087afd3f-99d7-45f0-8454-518ca4eb479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421aff6-d5a5-41ed-8e2a-58000e89776f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Opening multiple files with `xarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a0d0f-ef43-4531-9b05-8f73444fa61b",
   "metadata": {},
   "source": [
    "Now, let's switch back to `xarray`.\n",
    "\n",
    "**Again, we have several files representing different points in time.** Instead of writing a `for` loop, this time we'll use `open_mfdataset()` (\"open multi-file dataset\") to collect all the files into a single dataset. \n",
    "\n",
    "The string `'data_raw/NLDAS/*.nc'` describes where the files we want to open are located, where `*` is a wildcard: a symbol matching any number of text characters that may be present in a filename. In this case, we want to open *all* the netCDF files (`*.nc`) in the `data_raw/NLDAS` directory that contain the word `\"NOAH\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f86436-943d-4e04-afb4-eef26b55efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all the files as a single xarray Dataset\n",
    "ds = xr.open_mfdataset('data_raw/NLDAS/*NOAH*.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4da220-aaf2-4eac-8590-db8ff4fbb46f",
   "metadata": {},
   "source": [
    "**These NLDAS files contain multiple different variables...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9269f-b60c-4e56-8dbd-f60b5a8c24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14855d40-79e6-4aa2-872e-76c4ec5afff7",
   "metadata": {},
   "source": [
    "**In this case study, we're primarily interested in the variables that quantify the state of the water cycle or evaporative stress:**\n",
    "\n",
    "- `Evap`: This is total evapotranspiration\n",
    "- `SWdown`: Down-welling short-wave radiation, i.e., the amount of solar radiation directed downwards\n",
    "- `SMAvail_0_100cm`: The total liquid water in the top 100 cm of soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94adc4-98f7-4f1f-99f7-64043d40aac3",
   "metadata": {},
   "source": [
    "Now we've seen one of the big advantages of the `xarray` library: `xarray` already knows how netCDF variables should be displayed. It is capable of figuring out, based on the coordinates, how the image should be oriented.\n",
    "\n",
    "Below, because our dataset, `ds`, has more than one time step, we use the notation `ds['Evap'][0]` to subset the data to the first (zeroth) time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec11f90-a768-41e4-85ed-016a32382805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image in the time series\n",
    "ds['Evap'][0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ba162-104c-4de3-ac8d-7f17cdc318af",
   "metadata": {},
   "source": [
    "**However, if we extract a netCDF variable as a NumPy array, it will still be upside-down:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e87ed1-de0d-4f63-b420-a9b11dc2a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = ds['Evap'][:]\n",
    "\n",
    "pyplot.imshow(array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a60e1-9815-42ef-b1d3-d0cbf7d5ac92",
   "metadata": {},
   "source": [
    "This is why we must be careful when working with netCDF data, regardless of whether we use the `xarray` or `netCDF4` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094b6e3-21c6-43a2-8257-6d998d4843c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing a climatology\n",
    "\n",
    "**What distinguishes a flash drought or any drought from non-drought conditions?** We know that drought is characterized by reduced precipitation, reduced soil moisture, or both, but what is the magnitude of the reduction? To answer this question, we'd need to compare \"drought conditions\" to \"average conditions.\" That is, compared to a *long-term average,* what is the magnitude of the change in a meteorological condition, like monthly precipitation?\n",
    "\n",
    "The *long-term average* of a climate variable, calculated for some recurring interval (days, months, years), is called a **climatology.** In this case study, we're interested in how severe the August 2017 drought conditions were. We could quantify that by computing an August evapotranspiration climatology, which is the average August evapotranspiration (ET) over a long period of record/\n",
    "\n",
    "In this case, we have 10 years of monthly August ET, as indicated by the first axis of our array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f516f-c651-4c2a-8a2e-77096275536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Evap'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77700e-4080-4276-9213-0afc7265493f",
   "metadata": {},
   "source": [
    "Computing a climatology, therefore, is as easy as calling `mean()` on our array and averaging over the `'time'` axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16034944-d6bd-4f1f-87c8-03be26e45595",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clim = ds['Evap'].mean('time')\n",
    "et_clim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2949fe-b779-4009-8b26-52e807693bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We're flipping the et_clim array upside-down because of the CF convention\n",
    "pyplot.imshow(np.flipud(et_clim))\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration [kg m-2]')\n",
    "pyplot.title('Mean August ET')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778d73f-1c62-45d4-99fa-650972f0a9c4",
   "metadata": {},
   "source": [
    "### Describing climatic extremes using anomalies\n",
    "\n",
    "To figure out how much lower August ET was in 2017, we want to subtract the climatology from the August 2017 ET, effectively removing the average ET and showing only deviations from (above or below) the mean.\n",
    "\n",
    "When we subtract the mean from a time series, the result is often called the **anomaly** (deviation from the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76515bd-32a7-4a9e-9335-43566d6e19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_2017_anomaly = ds['Evap'][-1] - et_clim\n",
    "\n",
    "pyplot.imshow(np.flipud(et_2017_anomaly), cmap = 'RdYlBu')\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration Anomaly [kg m-2]')\n",
    "pyplot.title('August 2017 ET Anomaly')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fe09f-76f6-4969-85ce-ad1726c7ef5b",
   "metadata": {},
   "source": [
    "We can compute the anomalies in *every year* by simply writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592df46b-5667-4ebc-b015-2142b21afbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_anomaly = ds['Evap'] - et_clim\n",
    "et_anomaly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132ea5f-062f-4255-bdd0-0946953410cd",
   "metadata": {},
   "source": [
    "#### Using `cartopy`\n",
    "\n",
    "Once again, it can be helpful to use `cartopy` to see our data in context. In this example, we use `cartopy` built-in support for [data from Natural Earth](https://www.naturalearthdata.com/) to see the U.S. state boundaries on top of our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69040f-e17b-4168-bea9-b703014e1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [\n",
    "    ds['lon'].to_numpy().min(),\n",
    "    ds['lon'].to_numpy().max(),\n",
    "    ds['lat'].to_numpy().min(),\n",
    "    ds['lat'].to_numpy().max()\n",
    "]\n",
    "extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f587d-58fb-4e91-a9e4-4f895f00047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "shapename = 'admin_1_states_provinces_lakes'\n",
    "states_shp = shpreader.natural_earth(resolution = '110m', category = 'cultural', name = shapename)\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(1, 1, 1, projection = ccrs.PlateCarree())\n",
    "ax.imshow(np.flipud(et_2017_anomaly), extent = extent, cmap = 'RdYlBu')\n",
    "ax.add_geometries(shpreader.Reader(states_shp).geometries(), ccrs.PlateCarree(), facecolor = 'none')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d149745-03e3-414a-bfe6-bc57e1eb916e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Writing re-useable code and documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bed11-92ea-4585-b7d3-1a17a7761296",
   "metadata": {},
   "source": [
    "It looks like August 2017 was characterized by anomalously low evapotranspiration (ET) rates in the Northern Plains. What about other climate variables?\n",
    "\n",
    "We did a lot of work to create the ET anomaly plot. Is there a way we can easily apply the same workflow to our other climate variables?\n",
    "\n",
    "**This is what we create Python functions for: to automate a task in a consistent way.** Let's create a function for our current workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50353d6-6bb7-49db-8253-927edee9c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomaly(data):\n",
    "    '''\n",
    "    Calculates the anomaly in a long-term time series dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xarray.DataArray\n",
    "        The time-series data, a (T x M x N) array where T is the \n",
    "        number of time steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        The anomaly values\n",
    "    '''\n",
    "    clim = data.mean('time')\n",
    "    # Create a sequence of 2D maps, one for each year\n",
    "    return data - clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f5ba98-72a4-49a1-b0a9-db12e0a7d736",
   "metadata": {},
   "source": [
    "**The multi-line Python string (beginning with three quote characters, `'''`) marks the beginning of a Python *docstring* or documentation string.** The docstring must immediately follow the first line of the function definition.\n",
    "\n",
    "The docstring is what users see when they call for `help()` on your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b106e-c301-4520-81c2-fde2852812c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(calculate_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276e275-fab0-487c-a446-59999e66a8ed",
   "metadata": {},
   "source": [
    "**A good docstring tells the user:**\n",
    "\n",
    "- The purpose of the function; in our example, the function \"Generates a time series for a given variable...\"\n",
    "- What input *parameters* (arguments) the function accepts.\n",
    "- What the *return value* of the function is.\n",
    "\n",
    "It might also include one or more example use cases. The format of our docstring's \"Parameters\" and \"Return\" value are based on a convention (\"numpydoc\") and [you can read about that convention and alternatives at this reference.](https://pdoc3.github.io/pdoc/doc/pdoc/#supported-docstring-formats) Under the \"Parameters\" heading, we indicate the name of an input parameter, its type(s), and a brief explanation of what it means:\n",
    "\n",
    "```\n",
    "Parameters\n",
    "----------\n",
    "param_name : type\n",
    "    Indented 4 spaces, we describe the input parameter on the next line\n",
    "```\n",
    "\n",
    "The \"Returns\" heading is formatted in a similar way, except the return value doesn't have a name:\n",
    "\n",
    "```\n",
    "Returns\n",
    "-------\n",
    "type\n",
    "    Indented 4 spaces, we describe the output parameter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3433a06-45a1-43df-bbc1-ac9ff13a1d2f",
   "metadata": {},
   "source": [
    "**Most importantly, does our Python function work the way we expect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce74ca-5462-4d6a-acbd-22eb7a2385c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_anomaly = calculate_anomaly(ds['Evap'])\n",
    "pyplot.imshow(np.flipud(et_anomaly[-1]), cmap = 'RdYlBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a82c5-5681-4358-b0e2-5fdeba96665f",
   "metadata": {},
   "source": [
    "### Repeating our climate data analysis\n",
    "\n",
    "**Once we've written re-useable Python functions like this, we can begin to scale-up our analysis in new ways!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2153-a470-4353-a460-8ebe3cbacb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_anomaly = calculate_anomaly(ds['SWdown'])\n",
    "sm_anomaly = calculate_anomaly(ds['SMAvail_0_100cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebf2c2-2e78-4875-91e7-2cc212a82ff2",
   "metadata": {},
   "source": [
    "The same functions we used for calculating an ET anomaly can be used to calculate an anomaly for any variable we're interested in! What do the anomalies in solar radiation (`\"SWdown\"`) and soil moisture (`\"SMAvail_0_100cm\"`) look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be24e1-4f85-4155-bc9a-9b59a1a071ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    et_anomaly[-1],\n",
    "    rad_anomaly[-1],\n",
    "    sm_anomaly[-1]\n",
    "]\n",
    "labels = ['ET', 'Radiation', 'Soil Moisture']\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 5))\n",
    "ax = fig.subplots(1, 3)\n",
    "for i in range(3):\n",
    "    ax[i].imshow(np.flipud(images[i]), cmap = 'RdYlBu')\n",
    "    ax[i].set_title(labels[i] + ' Anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb09626-5186-4547-b1c5-74ed46ed24be",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Computing vapor pressure deficit\n",
    "\n",
    "One aspect of the climate system we haven't yet examined is **vapor pressure deficit (VPD),** which is a measure of how dry the air is. VPD tells us the amount of additional water (in terms of vapor pressure) that the air could hold at its current temperature. Under high VPD, the atmosphere can act as a drinking straw, drawing water away from the Earth's surface and from plants. Did anomalously high VPD play a role in the 2017 Northern Plains flash drought?\n",
    "\n",
    "VPD isn't available as a climate variable in the NLDAS re-analysis dataset, but we can compute it from the variables that are available. To do that, we'll need to download a slightly different NLDAS data collection, consisting of the atmospheric forcing data that drive the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181ac3e-a9d3-4cae-a903-a0244980fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# Get data from August for every year from 2008 up to (but not including) 2018\n",
    "for year in range(2008, 2018):\n",
    "    search = earthaccess.search_data(\n",
    "        short_name = 'NLDAS_FORA0125_M',\n",
    "        version = '2.0',\n",
    "        temporal = (f'{year}-08', f'{year}-08'))\n",
    "    results.extend(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec2fb8-810c-47ea-80dd-1410cd65dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(results, 'data_raw/NLDAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84897fb-4873-488f-9b69-6b45ab3f4370",
   "metadata": {},
   "source": [
    "Once again, we want to open multiple files as a single `xarray.Dataset` using `open_mfdataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6926bd1-2e59-4fb3-a06c-1b8ed8dbf3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset('data_raw/NLDAS/NLDAS_FORA*.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360342e9-b690-4fc9-8753-f59967348cf9",
   "metadata": {},
   "source": [
    "The NLDAS data we downloaded has three variables we're interested in:\n",
    "\n",
    "- `Tair`, the air temperature in degrees Kelvin\n",
    "- `Qair`, the specific humidity\n",
    "- `PSurf`, the near-surface air pressure in Pascals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eea2c68-f336-4a86-9358-d1ed98807686",
   "metadata": {},
   "source": [
    "### Challenge: Write a function to compute VPD\n",
    "\n",
    "VPD is defined as the difference between the saturation vapor pressure (SVP) and the actual vapor pressure (AVP). That is, it is the difference between how much water the air *could* hold at its current temperature and the actual amount of water it currently holds.\n",
    "$$\n",
    "\\text{VPD} = \\text{SVP} - \\text{AVP}\n",
    "$$\n",
    "\n",
    "The August-Roche-Magnus formula is a good approximation for SVP:\n",
    "\n",
    "$$\n",
    "\\text{SVP} = 610.94\\times \\text{exp}\\left(\n",
    "\\frac{17.625\\times T}{T + 243.04}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "And an approximation for AVP is given by Gates (1980, *Biophysical Ecology*):\n",
    "\n",
    "$$\n",
    "\\text{AVP} = \\frac{Q\\times P}{0.622 + (0.379\\times Q)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $T$ is the air temperature in degrees Kelvin\n",
    "- $Q$ is the specific humidity\n",
    "- $P$ is the air pressure in Pascals\n",
    "- VPD, SVP, and AVP are also in Pascals\n",
    "- $\\text{exp}$ refers to the exponential function and is available in NumPy as `np.exp()`\n",
    "\n",
    "**Write a Python function called `vpd()` to compute VPD.** When writing your function, remember:\n",
    "\n",
    "- Write an informative docstring!\n",
    "- Be sure to add inline comments to describe complex or potentially confusing code.\n",
    "- Consider what your variable names should be and how you might use them to communicate measurement units.\n",
    "- When you've finished, compare it to the one written below.\n",
    "\n",
    "**Hint:** You'll need to convert air temperature from degrees Kelvin to degrees Celsius by subtracting 273.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17942f63-42de-4f9f-87fb-ac66c07a8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vpd(temp_k, pressure_pa, s_humidity):\n",
    "    '''\n",
    "    Computes vapor pressure deficit (VPD).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    temp_c : xarray.DataArray\n",
    "        Air temperature in degrees Kelvin\n",
    "    pressure_pa : xarray.DataArray\n",
    "        Air pressure in Pascals\n",
    "    s_humidity : xarray.DataArray\n",
    "        Specific humidity (dimensionless)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        VPD in Pascals\n",
    "    '''\n",
    "    temp_c = temp_k - 273.15\n",
    "    # Saturation vapor pressure (Pa)\n",
    "    svp = 610.94 * np.exp((17.625 * temp_c) / (temp_c + 243.04))\n",
    "    # Actual vapor pressure (Pa)\n",
    "    avp = (s_humidity * pressure_pa) / (0.622 + (0.379 * s_humidity))\n",
    "    return svp - avp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67872e3-2180-465e-848c-f48bb18ec095",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that you've written a function to compute VPD, let's apply it to our NLDAS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54261fe6-0b9e-4b44-99d2-0b07205be586",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpd_series = vpd(ds['Tair'], ds['PSurf'], ds['Qair'])\n",
    "\n",
    "vpd_anomaly = calculate_anomaly(vpd_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4639e6-d3cd-47f2-abd7-0f06038b68cd",
   "metadata": {},
   "source": [
    "From the plot below, it appears that only part of the Northern Plains experienced above-average VPD in August 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8447780-7792-4189-97bc-07e665cfd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(np.flipud(vpd_anomaly[-1]), cmap = 'RdYlBu')\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('VPD Anomaly (Pa)')\n",
    "pyplot.title('August 2017 VPD Anomaly')\n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d07cdaef-4638-4714-8cab-1245d82c7e55",
   "metadata": {},
   "source": [
    "### Saving our results\n",
    "\n",
    "We've done a lot of interesting work with the NLDAS data. What if we wanted to save our results for someone else to use? What might they need to know about what we've done, and how could we communicate that?\n",
    "\n",
    "This is where a **self-documenting** file format like netCDF can help!\n",
    "\n",
    "Starting with our `xarray.DataArray`, let's add some **metadata** in the form of **attributes.** What are some things that people should know about these data? The measurement units are important, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf73486-360a-4704-a12c-c467dd9df397",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpd_anomaly.attrs['units'] = 'Pascals'\n",
    "vpd_anomaly.attrs['name'] = 'Vapor pressure deficit anomaly, relative to 2008-2017 climatology'\n",
    "vpd_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74482dc-41f8-4d9f-8895-bccb08627042",
   "metadata": {},
   "source": [
    "We're now ready to create our new `xarray.Dataset`!\n",
    "\n",
    "The `xr.Dataset()` constructor function takes at least two arguments:\n",
    "\n",
    "- The data variables, usually in the form a Python dictionary with key-value pairs representing the variable name (key) and the `DataArray` (value).\n",
    "- The coordinates of the `DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546cebe-d42b-4d06-8a94-b1ba5221ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = xr.Dataset({'vpd_anomaly': vpd_anomaly}, ds.coords)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53a991-2f2a-462c-b520-7d78e785f0cd",
   "metadata": {},
   "source": [
    "We can also add some **file-level attributes.** If we ever wanted to change anything about this dataset, we might want to know what Python script was used to create it in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a24861-55c2-4036-93f1-3d7212929eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.attrs['source_file'] = 'CaseStudy_2017_Northern_Plains_Flash_Drought.ipynb'\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c920752-832a-4d63-9801-ece1b88737f5",
   "metadata": {},
   "source": [
    "**Finally, we're ready to write our file to disk! But what filename should we choose?** For derived outputs, we should pick something *meaningful* that tells us information about:\n",
    "\n",
    "- What kind of data the file contains\n",
    "- What spatial locations or time periods it pertains to\n",
    "- What source data were used\n",
    "\n",
    "We'll also make sure to put the file in our `outputs` folder so that we don't mistake it for raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70073e-7912-4fbc-a4c9-e7dfc81b2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.to_netcdf('outputs/NLDAS_VPD_anomalies_2008-2017.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f04ad-61cd-40c5-8db1-a218161b48ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## More resources\n",
    "\n",
    "- Curious about how to use `earthaccess.open()` along with `xarray` so that you don't have keep any downloaded files around? Well, `xarray.open_dataset()` can be slow when you have a lot of files to open, as in this time-series example. [This article describes how you can speed up `xarray.open_dataset()`](https://climate-cms.org/posts/2018-09-14-dask-era-interim.html) when working with multiple cloud-hosted files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ae193-7a05-4bde-8264-209483e74743",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Chen, L. G., J. Gottschalck, A. Hartman, D. Miskus, R. Tinker, and A. Artusa. 2019. Flash drought characteristics based on U.S. Drought Monitor. Atmosphere 10 (9):498.\n",
    "- He, M., J. S. Kimball, Y. Yi, S. W. Running, K. Guan, K. Jensco, B. Maxwell, and M. Maneta. 2019. Impacts of the 2017 flash drought in the US Northern plains informed by satellite-based evapotranspiration and solar-induced fluorescence. Environmental Research Letters 14 (7):074019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
