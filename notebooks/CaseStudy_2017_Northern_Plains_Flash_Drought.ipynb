{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79bde19-ec9b-401e-8ad0-6bfad1ad6dc3",
   "metadata": {},
   "source": [
    "# Case Study: 2017 Northern Plains Flash Drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c13a88-9869-4dad-8384-e863e384589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7b524-c0a6-46d7-9a8c-255d2a9a5c55",
   "metadata": {},
   "source": [
    "## Before We Get Started\n",
    "\n",
    "For this case study, we're going to download some data from [the North American Land Data Assimilation System (NLDAS).](https://disc.gsfc.nasa.gov/datasets/NLDAS_NOAH0125_M_2.0/summary?keywords=NLDAS)\n",
    "\n",
    "Consequently, we'll need a place to store these raw data. It's important that we have a folder in our file system reserved for these raw data so we can keep them separate from any new datasets we might create. \n",
    "\n",
    "**Let's create a folder called `data_raw` in our Jupyter Notebook's file system.**\n",
    "\n",
    "We should never modify the raw data (that we're about to download). Doing so would make it hard to repeat the analysis we're going to perform as we will lose the original data values. This doesn't mean we have to keep the `data_raw` folder around forever: if it's publicly available data, we can always download it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22575e4c-dada-4ebe-8d8e-67b5f55fc39d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cde0a-21a3-4a6d-bc78-78aa58b51804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Show how to find the \"short_name\" and \"version\"\n",
    "# TODO Compare to the pattern for downloading a single granule (single date)\n",
    "# TODO Show how string formatting works\n",
    "\n",
    "results = []\n",
    "\n",
    "for year in range(2008, 2018):\n",
    "    search = earthaccess.search_data(\n",
    "        short_name = 'NLDAS_NOAH0125_M',\n",
    "        version = '2.0',\n",
    "        temporal = (f'{year}-08', f'{year}-08'))\n",
    "    results.extend(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f8847-6347-4ad7-8179-de355a4ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12eab9d-a5d7-4eef-8191-140c30c037e8",
   "metadata": {},
   "source": [
    "Previously, we've used `earthaccess.open()` to get access to these data. This time, we'll use `earthaccess.download()`. What's the difference?\n",
    "\n",
    "- `earthaccess.open()` provides a file-like object that is available to be downloaded and read *only we need it.*\n",
    "- `earthaccess.download()` actually downloads the file to our file system.\n",
    "\n",
    "**Note that, below, we're telling `earthaccess.download()` to put the downloaded files into our new `data_raw` folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3692c6-ae6c-4f6d-b919-747917eb9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(results, 'data_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f97ffe-29fe-4a4c-9c14-555aedd29d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('data_raw/*.nc')\n",
    "file_list.sort()\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f86436-943d-4e04-afb4-eef26b55efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "\n",
    "# Open just the first file\n",
    "nc = netCDF4.Dataset(file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad44b04-3ef4-4497-b056-7b8568156123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Discuss file-level metadata\n",
    "\n",
    "nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfff4f-9a2a-486a-a89d-44a477612ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Discuss file-level metadata\n",
    "# TODO Discuss \"scale_factor\" and \"add_offset\" and \"missing_value\"\n",
    "\n",
    "et = nc.variables['Evap']\n",
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0e2bd-12db-432f-af8c-f684d98b73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note the shape\n",
    "# TODO Note the orientation\n",
    "# TODO Discuss CF convention\n",
    "\n",
    "pyplot.imshow(et[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dd59c-b288-41c3-818c-2039cda5c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(np.flipud(et[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087afd3f-99d7-45f0-8454-518ca4eb479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note data type, why we're changing it to an array\n",
    "\n",
    "type(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9f59c-aa52-4295-a631-be5bb40f8a4b",
   "metadata": {},
   "source": [
    "### Opening netCDF4 Data with `xarray`\n",
    "\n",
    "Instead of using the `netCDF4` module, we can use `xarray` to open netCDF4 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfe640-ab4c-4e37-b20d-ea5a398fa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.open_dataset(file_list[0])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43346597-b520-4916-82a1-99de4c5f286f",
   "metadata": {},
   "source": [
    "A big advantage to using `xarray` is how it organizes all the information we're interested in. Recall that `xarray` variables can be accessed using a dictionary-like indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85288d-7145-42b4-9412-5946f1a36d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Evap']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94adc4-98f7-4f1f-99f7-64043d40aac3",
   "metadata": {},
   "source": [
    "Another advantage is that `xarray` already knows how these netCDF4 variables should be displayed; its capable of figuring out, based on the coordinates, how the image should be oriented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec11f90-a768-41e4-85ed-016a32382805",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Evap'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c09e26-621e-477e-b66b-047da950470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = dataset['Evap'].to_numpy()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df262edf-40a7-411d-a303-8f35957935b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e87ed1-de0d-4f63-b420-a9b11dc2a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8dab3e-a300-4284-8e56-f6967eb331d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_series = []\n",
    "\n",
    "for filename in file_list:\n",
    "    ds = xr.open_dataset(filename)\n",
    "    et = ds['Evap'].to_numpy()\n",
    "    # Don't forget to to flip the image upside-down!\n",
    "    et_series.append(np.flipud(et[0]))\n",
    "\n",
    "et_series = np.stack(et_series, axis = 0)\n",
    "et_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094b6e3-21c6-43a2-8257-6d998d4843c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing a Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b5090-84d5-4dc7-9115-71f70d7d987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define a climatology\n",
    "\n",
    "et_clim = et_series.mean(axis = 0)\n",
    "et_clim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a994a3-f797-40ec-984c-8aed3dee14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(et_clim)\n",
    "pyplot.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c32075-89bc-4c5a-b3b8-a84042338fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO NoData\n",
    "\n",
    "et_clim.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2949fe-b779-4009-8b26-52e807693bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clim[et_clim < 0] = np.nan\n",
    "\n",
    "pyplot.imshow(et_clim)\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration [kg m-2]')\n",
    "pyplot.title('Mean September ET')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778d73f-1c62-45d4-99fa-650972f0a9c4",
   "metadata": {},
   "source": [
    "### How Does September 2017 Compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd8479-61eb-4cf6-9550-556df714f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76515bd-32a7-4a9e-9335-43566d6e19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_2017_anomaly = et_series[-1] - et_clim\n",
    "\n",
    "pyplot.imshow(et_2017_anomaly, cmap = 'RdYlBu')\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration Anomaly [kg m-2]')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132ea5f-062f-4255-bdd0-0946953410cd",
   "metadata": {},
   "source": [
    "#### Using `cartopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69040f-e17b-4168-bea9-b703014e1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [\n",
    "    nc.variables['lon'][:].min(),\n",
    "    nc.variables['lon'][:].max(),\n",
    "    nc.variables['lat'][:].min(),\n",
    "    nc.variables['lat'][:].max()\n",
    "]\n",
    "extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f587d-58fb-4e91-a9e4-4f895f00047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "shapename = 'admin_1_states_provinces_lakes'\n",
    "states_shp = shpreader.natural_earth(resolution = '110m', category = 'cultural', name = shapename)\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(1, 1, 1, projection = ccrs.PlateCarree())\n",
    "ax.imshow(et_2017_anomaly, extent = extent, cmap = 'RdYlBu')\n",
    "ax.add_geometries(shpreader.Reader(states_shp).geometries(), ccrs.PlateCarree(), facecolor = 'none')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4d38e-bc5a-46c4-8ba6-08bedd709515",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Saving Our Reproducible Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074ddf4-7c27-4b4f-a82b-754139b12876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Outline the steps in our workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e0eac-8a7d-46d9-83d3-0c98b982ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Discuss docstring\n",
    "\n",
    "def stack_time_series(netcdf_file_list, variable, nodata = -9999):\n",
    "    '''\n",
    "    Generates a time series for a given variable, based on an \n",
    "    ordered list of netCDF4 files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    netcdf_file_list : list\n",
    "        The list of netCDF4 files, where each file represents a date\n",
    "    variable : str\n",
    "        The name of the variable of interest\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    '''\n",
    "    series = []\n",
    "    for filename in file_list:\n",
    "        ds = xr.open_dataset(filename)\n",
    "        et = ds['Evap'].to_numpy()\n",
    "        # Don't forget to to flip the image upside-down!\n",
    "        et_series.append(np.flipud(et[0]))\n",
    "    \n",
    "    series = np.stack(series, axis = 0)\n",
    "    # Fill in the NoData values\n",
    "    series[series == nodata] = np.nan\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf0a13-788e-43d9-8968-acf773a1d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "et = stack_time_series(file_list, 'Evap')\n",
    "et.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69ab3b-f7b5-40ce-a37a-202976043b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "et.mean(axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285dd31-d7a3-4a3e-8be8-f6839de17efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Discuss broadcasting\n",
    "\n",
    "anomaly = et - et.mean(axis = 0)\n",
    "anomaly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94718bdc-ba67-4a22-b8c3-d106a3d0f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write docstring together with learners\n",
    "\n",
    "def anomalies(time_series):\n",
    "    '''\n",
    "    Computes the anomaly (current value minus mean value) in a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series : numpy.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    '''\n",
    "    clim = time_series.mean(axis = 0)\n",
    "    return time_series - clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2deda0-8988-45a2-89f5-4b92acfd0863",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2153-a470-4353-a460-8ebe3cbacb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('data_raw/*.nc')\n",
    "\n",
    "et = stack_time_series(file_list, 'Evap')\n",
    "et_anomaly = anomalies(et)\n",
    "\n",
    "rad_anomaly = anomalies(stack_time_series(file_list, 'SWdown'))\n",
    "sm_anomaly = anomalies(stack_time_series(file_list, 'SMAvail_0_100cm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be24e1-4f85-4155-bc9a-9b59a1a071ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    et_anomaly[-1],\n",
    "    rad_anomaly[-1],\n",
    "    sm_anomaly[-1]\n",
    "]\n",
    "labels = ['ET', 'Radiation', 'Soil Moisture']\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 5))\n",
    "ax = fig.subplots(1, 3)\n",
    "for i in range(3):\n",
    "    ax[i].imshow(images[i], cmap = 'RdYlBu')\n",
    "    ax[i].set_title(labels[i] + ' Anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762ecbd-552c-4892-a296-2292906776a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bringing in NASA Earth Observations\n",
    "\n",
    "The NLDAS data we've used are a great tool for retrospective studies but, as a re-analysis dataset, it has some limitations:\n",
    "\n",
    "- It has a relatively high latency; it may be days or weeks before data are available.\n",
    "- It integrates data from multiple sources but with varying levels of accuracy and geographic coverage.\n",
    "\n",
    "If we want to characterize flash drought or detect it in near-real time, we shouldn't use re-analysis datasets. Instead, we want some kind of direct observation of drought conditions. **Let's see what we can learn about the 2017 Flash Drought from NASA's satellite-based soil-moisture estimates.**\n",
    "\n",
    "**We'll use data from NASA's Soil Moisture Active Passive (SMAP) Mission.** [NASA's earth observing missions provide data that is grouped into different processing levels:](https://www.earthdata.nasa.gov/engage/open-data-services-and-software/data-information-policy/data-levels)\n",
    "\n",
    "- **Level 1 (Raw data):** Basically, these are data values measured directly by a satellite instrument. They may or may not be physically interpretable. Most end-users won't benefit from Level 1 data.\n",
    "- **Level 2:** These are physically interpretable values that have been derived from the raw data, at the same spatial and temporal resolution as the Level 1 data. Level 2 data may be hard to use because the spatial structure of the data matches the instrument's viewing geometry.\n",
    "- **Level 3:** At Level 3, the geophysical values have been standardized on a uniform spatial grid and uniform time series. While some values may be missing due to low quality, clouds, or sensor failure, gridded Level 3 data from different time steps can be easily combined and compared.\n",
    "- **Level 4 (Model-enhanced data):** At Level 4, the values from Level 3 data are incorporated into some kind of model, possibly combining additional, independent datasets from other sensors in order to produce enhanced estimates or analyses of geophysical variables.\n",
    "\n",
    "### Downloading the Data\n",
    "\n",
    "**[We'll use the 36-km Level 3 surface soil moisture data from the SMAP mission](https://nsidc.org/data/spl3smp/versions/8)** because these are a good compromise between direct sensor observations and ease of use.\n",
    "\n",
    "- At the website above, we can see there are multiple ways of accessing the data. [Let's use Earthdata Search;](https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008) can we access the data from NASA's cloud using `earthaccess`?\n",
    "- You may have noticed that the Level 3 SMAP data we want to use are *not* \"Available in Earthdata Cloud.\" It looks like we'll have to download the data directly.\n",
    "- **Where will we put the raw data we download?** Let's revisit our file tree in Jupyter Notebook.\n",
    "- **Within the `data_raw` folder, let's create a new folder called `SMAP_L3`.** This is where we'll put the data we're about to download.\n",
    "\n",
    "We've discussed the importance of having a well-documented workflow that makes it easy to understand how we obtained a particular scientific result. We assume that we can re-download the raw data we used anytime, but what if we forget where the data came from? Since the SMAP Level 3 data aren't available in the Cloud, we're about to do download the data manually, and it would be a good idea to document what steps we took to do that, in case there are questions about where the data came from or what kind of processing was applied.\n",
    "\n",
    "- In the Jupyter Notebook file tree, within the `SMAP_L3` folder let's make a new `\"New File\"`. Name the new text file `README.txt`.\n",
    "- Double-click `README.txt` to open it. This is where we'll add some useful information about the data we're about to download. Below is an example.\n",
    "\n",
    "```\n",
    "Author: K. Arthur Endsley\n",
    "Date: November 1, 2023\n",
    "\n",
    "This folder contains Level 3 data from the SMAP Mission. It was downloaded from:\n",
    "\n",
    "    https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008\n",
    "\n",
    "Here's some more information about this product:\n",
    "\n",
    "    https://nsidc.org/data/spl3smp/versions/8\n",
    "```\n",
    "\n",
    "This might not seem like a lot of information but there's plenty here that we would want to know if we took a long break from this project or if someone else had to try and figure out what we were doing. And its short length is also an advantage: **documenting your project doesn't have to be hard and any amount of information is better than none.**\n",
    "\n",
    "### Customizing an Earthdata Search Download\n",
    "\n",
    "The SMAP satellite has two overpasses every day, a \"morning\" and an \"afternoon\" overpass (local time). Let's use soil moisture data from the afternoon (PM) overpass, because this is likely when soil moisture stress on vegetation is at its peak.\n",
    "\n",
    "- We'll download data from August and September to study the onset and progression of the 2017 Flash Drought: **Choose a temporal subset, 2017-08-01 through 2017-09-30.**\n",
    "- At the bottom right, **click the big green button that reads \"Download All.\"**\n",
    "- 1.9 GB is a lot of data! Can we make this download any smaller? We're only interested in soil moisture from the afternoon overpass. **Click \"Edit Options\" and under \"Select a data access method,\" select the \"Customize\" option.**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order.png)\n",
    "\n",
    "- **Scroll down to \"Configure data customization options\" and down to \"Band subsetting.\"**\n",
    "- **Within the text box that reads \"Filter\" type `soil_moisture_dca_pm`.** This will filter the available variables (\"bands\") to just this specific variable, which is the soil moisture estimate from the Dual-Channel Algorithm (DCA) for the afternoon (PM) overpass.\n",
    "- To make sure that `soil_moisture_dca_pm` is the *only* variable we download, **you'll need to uncheck the box next to `SPL3SMP` then re-check the box next to `soil_moisture_dca_pm` (see screenshot below).**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order_variables.png)\n",
    "  \n",
    "- Hit \"Done\" at the bottom of this form then the big green button that reads \"Download Data\"!\n",
    "\n",
    "#### But Wait!\n",
    "\n",
    "Because we selected a subset of variables, we'll have to wait to get an e-mail that the order is ready. **You don't need to do these steps yourself, because I already prepared all the data granules that would be downloaded this way.** They can be download directly from here:\n",
    "\n",
    "- [SMAP_L3_SPL3SMP_V008_20170801_20170930.zip](http://files.ntsg.umt.edu/data/ScienceCore/SMAP_L3_SPL3SMP_V008_20170801_20170930.zip) (Extract this ZIP file's contents to your `data_raw/SMAP_L3` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fcfd0-56f9-4ad7-9998-4104e4680432",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Reading SMAP Level 3 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae66ba-1f3f-4ead-a899-b68503ae252d",
   "metadata": {},
   "source": [
    "The SMAP Level 3 data we downloaded are each stored as a **Hierarchical Data File, version 5 (HDF5).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af777f84-05bf-4a48-b6d7-11ee827faf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "hdf = h5py.File('data_raw/SMAP_L3/SMAP_L3_SM_P_20170801_R18290_001_HEGOUT.h5', 'r')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc21c83-44dc-4ef4-a7a6-dad557c0241c",
   "metadata": {},
   "source": [
    "An HDF5 file is a lot like a netCDF4 file: they are both hierarhical files capable of storing multiple, diverse datasets and metadata in a single file. What do we mean by \"hierarchical\"? Well, an HDF5 or netCDF4 file is like a file tree, where *datasets* can be organized into different nested *groups,* as depicted below. Metadata, in the form of *attributes,* can be attached to any dataset or group throughout the file.\n",
    "\n",
    "![](assets/hdf5-structure.jpg)\n",
    "\n",
    "*Image courtesy of NEON Science.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f478567-664d-48cf-990a-f38ad78fa1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73580d-bc31-4f59-8064-ccce0d0ef783",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9469661-4435-4472-b429-ccee11945eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cad114-a67c-45d4-8941-e5914205bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Significance of an empty group?\n",
    "hdf['Metadata/ProcessStep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc84ca9-828b-411c-ad3b-4330e30fa509",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66735587-3c8e-4572-a004-338c00222ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs['softwareTitle']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573327d-a61f-45fe-8b82-9c2598767731",
   "metadata": {},
   "source": [
    "### Reading HDF5 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23df3c-e8c9-4f72-8f6d-09b53342d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_raw/SMAP_L3/SMAP_L3_SM_P_20170801_R18290_001_HEGOUT.h5'\n",
    "\n",
    "hdf = h5py.File(filename, 'r')\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4937b54-50e1-43de-b426-23745f86caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Explain that we'd like to open the file with xarray\n",
    "# TODO Note there are no coordinates\n",
    "\n",
    "ds = xr.open_dataset(filename, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf531a-2dab-4067-8db3-5bdde91b4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note coordinates assignment\n",
    "\n",
    "ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa77a5-e5e3-4a5d-95ba-8c6fbd7a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Remark on striping\n",
    "\n",
    "pyplot.figure(figsize = (12, 5))\n",
    "ds['soil_moisture_dca_pm'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64bda8-8d1d-4ef1-87a9-5fed52fd8a56",
   "metadata": {},
   "source": [
    "### Summary: Reading HDF5 and netCDF4 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b387aa-ec14-4496-a012-8bceff9b602c",
   "metadata": {},
   "source": [
    "|                              |  HDF5                              | netCDF4                                |\n",
    "|:-----------------------------|:-----------------------------------|:---------------------------------------|\n",
    "|Module name                   | `h5py`                             | `netCDF4`                              |\n",
    "|Files opened with...          | `hdf = h5py.File(...)`             | `nc = netCDF4.Dataset()`               |\n",
    "|Datasets/groups viewed with...| `hdf.keys()`                       | `nc.variables` or `nc.variables.keys()`|\n",
    "|                              | `hdf['group_name'].keys()`         | `nc.variables['group_name'].keys()`    |\n",
    "|Datasets accessed through...  | `hdf`                              | `nc.variables`                         |\n",
    "|Attributes listed through...  | `hdf['dataset'].attrs`             | `nc.variables['dataset'].ncattrs()`    |\n",
    "|Attributes read by...         | `hdf['dataset'].attrs['attribute']`| `nc.variables['dataset'].getncattr()`  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f04ad-61cd-40c5-8db1-a218161b48ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## More Resources\n",
    "\n",
    "- Curious about how to use `earthaccess.open()` along with `xarray` so that you don't have keep any downloaded files around? Well, `xarray.open_dataset()` can be slow when you have a lot of files to open, as in this time-series example. [This article describes how you can speed up `xarray.open_dataset()`](https://climate-cms.org/posts/2018-09-14-dask-era-interim.html) when working with multiple cloud-hosted files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
