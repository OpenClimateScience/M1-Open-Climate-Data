{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79bde19-ec9b-401e-8ad0-6bfad1ad6dc3",
   "metadata": {},
   "source": [
    "# M1.8 - Case Study: 2017 Northern Plains Flash Drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea7189-42c4-4f69-89c1-99053040b61b",
   "metadata": {},
   "source": [
    "Let's use everything we've learned so far to study a real drought event in an agricultural system.\n",
    "\n",
    "Whereas meteorological drought is characterized by a deficit of precipitation, **flash drought** is characterized by a sudden, extreme demand for water from the land surface: \"anomalously high evapotranspiration rates, caused by anomalously high temperatures, winds, and/or incoming radiation\" (Chen et al. 2019).\n",
    "\n",
    "In 2017, a flash drought emerged in the Northern Plains of the United States. The U.S. Drought Monitor estimated that, on September 5, 2017, about 23% of the region experienced \"extreme drought\" conditions and subsequent crop losses and water shortages (He et al. 2019, *Environmental Research Letters*).\n",
    "\n",
    "**For this case study, we're going to consider the following climate variables and data sources:**\n",
    "\n",
    "- Evapotranspiration, radiation, and soil moisture data from [the North American Land Data Assimilation System (NLDAS)](https://disc.gsfc.nasa.gov/datasets/NLDAS_NOAH0125_M_2.0/summary?keywords=NLDAS), a re-analysis dataset.\n",
    "- Soil moisture the Soil Moisture Active Passive (SMAP) mission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c13a88-9869-4dad-8384-e863e384589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7b524-c0a6-46d7-9a8c-255d2a9a5c55",
   "metadata": {},
   "source": [
    "## Organizing our file system\n",
    "\n",
    "We'll need a place to store these raw data. It's important that we have a folder in our file system reserved for these raw data so we can keep them separate from any new datasets we might create. \n",
    "\n",
    "**Let's create a folder called `data_raw` in our Jupyter Notebook's file system.** You can do this from the Jupyter Notebook home page (the file tree) by selecting \"New\" and then \"New Folder\" as in the screenshot below.\n",
    "\n",
    "![](./assets/M1_screenshot_Jupyter_new_folder.png)\n",
    "\n",
    "**Create two sub-folders within `data_raw`:**\n",
    "\n",
    "- `NLDAS`\n",
    "- `SMAP`\n",
    "\n",
    "We should never modify the raw data (that we're about to download). Doing so would make it hard to repeat the analysis we're going to perform as we will lose the original data values. This doesn't mean we have to keep the `data_raw` folder around forever: if it's publicly available data, we can always download it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22575e4c-dada-4ebe-8d8e-67b5f55fc39d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Downloading the NLDAS data\n",
    "\n",
    "We'll use the function `earthaccess.search_data()` again. In this case, the `short_name` and `version` can be found on [the Goddard Earth Sciences (GES) Data and Information Services Center (DISC) website for this product.](https://disc.gsfc.nasa.gov/datasets/NLDAS_NOAH0125_M_2.0/summary?keywords=NLDAS)\n",
    "\n",
    "**The NLDAS data we're interested in are compiled monthly and we want to download an August monthly dataset for each year.** Because the dates we want are non-consecutive, we need to call `earthaccess.search_data()` within a `for` loop. Below, we also use string formatting so that the string `f'{year}-08'` becomes, e.g.: `'2008-08'`, `'2009-08'`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cde0a-21a3-4a6d-bc78-78aa58b51804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor: Show how to find the \"short_name\" and \"version\"\n",
    "\n",
    "results = []\n",
    "\n",
    "# Get data from August for every year from 2008 up to (but not including) 2018\n",
    "for year in range(2008, 2018):\n",
    "    search = earthaccess.search_data(\n",
    "        short_name = 'NLDAS_NOAH0125_M',\n",
    "        version = '2.0',\n",
    "        temporal = (f'{year}-08', f'{year}-08'))\n",
    "    results.extend(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f8847-6347-4ad7-8179-de355a4ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12eab9d-a5d7-4eef-8191-140c30c037e8",
   "metadata": {},
   "source": [
    "Previously, we've used `earthaccess.open()` to get access to these data. This time, we'll use `earthaccess.download()`. What's the difference?\n",
    "\n",
    "- `earthaccess.open()` provides a file-like object that is available to be downloaded and read *only we need it.*\n",
    "- `earthaccess.download()` actually downloads the file to our file system.\n",
    "\n",
    "**Note that, below, we're telling `earthaccess.download()` to put the downloaded files into our new `data_raw` folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3692c6-ae6c-4f6d-b919-747917eb9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(results, 'data_raw/NLDAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcd7b2-9b50-4367-847e-dce4cae46fdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reading netCDF files without `xarray`\n",
    "\n",
    "Although we could open these netCDF files using `xarray`, we're going to first use a different Python library called `netCDF4`. This will help us learn more about the netCDF file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8743148-9ae6-434e-89bc-ee10f2e3296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "\n",
    "nc = netCDF4.Dataset('data_raw/NLDAS/NLDAS_NOAH0125_M.A200808.020.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0479a8-8cc0-4dee-bcf1-5003badaa8f1",
   "metadata": {},
   "source": [
    "As we previously discussed, one of the advantages of the netCDF file format is that it is **self-documenting;** there are file-level and dataset-level descriptive information, or metadata, called **attributes.**\n",
    "\n",
    "When using the `netCDF4` library to open a netCDF file, we can access **file-level attributes** this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47921a58-b40f-4fc9-8f57-99187054d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.ncattrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9202eb-b4f3-4772-a560-6ac83006ac7b",
   "metadata": {},
   "source": [
    "And we can access **dataset-level attributes** this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375fdb-7939-48c0-9b3c-76dde699858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc['Evap'].ncattrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf959b-7041-4f53-b031-f40ea60eb250",
   "metadata": {},
   "source": [
    "### Getting real values from netCDF datasets\n",
    "\n",
    "There are some significant differences between how the `xarray` and `netCDF4` libraries represent netCDF files. One important difference is how array data are read from the files.\n",
    "\n",
    "The `netCDF4` package reads array data from the file the way it is stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a08166-9af5-4359-a061-45df888bc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc['Evap']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66a146-d0ab-4674-b24f-2539a81f0a10",
   "metadata": {},
   "source": [
    "**Notice the `scale_factor`, `add_offset`, and `missing_value` attributes.** These are very important to consider because of the way netCDF4 files sometimes store variables. If the variables are packed in a certain way to save disk space, we need to transform the packed values into real values before using the data:\n",
    "\n",
    "$$\n",
    "\\text{Real value} = (\\text{Packed value}\\times \\text{Scale factor}) + \\text{Offset}\n",
    "$$\n",
    "\n",
    "**When we look at the `\"Evap\"` (total evapotranspiration) dataset's attributes with `xarray`, however...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfff4f-9a2a-486a-a89d-44a477612ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data_raw/NLDAS/NLDAS_NOAH0125_M.A200808.020.nc')\n",
    "\n",
    "ds['Evap'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb860609-7556-4dfd-868c-c353a791f81d",
   "metadata": {},
   "source": [
    "**Note that the attributes are different!** The `scale_factor`, `add_offset`, and `missing_value` attributes are missing.\n",
    "\n",
    "**This is because `xarray` transforms packed values into real values automatically for us.** Compare the two examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c1fa0-176f-4380-9209-ef5d3cbee4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(nc['Evap'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4e8c7-4d0b-4f3a-85d5-ff44bb342999",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ds['Evap'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0c869-d4ad-4c76-b7a6-cd0ad40e36e3",
   "metadata": {},
   "source": [
    "**In this case, the `scale_factor` is `1.0` and the `add_offset` is `0.0`, meaning the packed values are the same as the real values.** Hence, there is no difference in the numbers for valid data areas, above; but we can see that the `xarray.Dataset` replaced the `missing_value` (-9999) with `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3eea39-9872-42ce-be6e-958c471bee12",
   "metadata": {},
   "source": [
    "### Plotting netCDF4 Variables\n",
    "\n",
    "Let's plot the data from the first monthly dataset (August 2008). **Recall that, when using `pyplot.imshow()`, we have to provide a 2D array...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a17ca7-9a8c-4cb6-8068-55338d46db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "et = nc['Evap']\n",
    "\n",
    "et.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8d8b5-7d87-4464-93b4-69ce51a1f90b",
   "metadata": {},
   "source": [
    "The first axis of our `et` array is trivial, as it has only one element. We can simply subset the `et` array to this \"first\" (and only) element using `et[0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0e2bd-12db-432f-af8c-f684d98b73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(et[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f917ad-80e6-4c06-9fd1-904cd3bce7ee",
   "metadata": {},
   "source": [
    "Does this look right? Why is it upside down?\n",
    "\n",
    "The reason is because of [the CF Convention](http://cfconventions.org/) that defines how netCDF4 files should be formatted. Part of that standard requires that the coordinate arrays (here, latitude and longitude arrays) be sorted from smallest number to largest number. **Whereas spatial coordinate systems like latitude-longitude have numbers increasing from bottom-to-top and left-to-right, image coordinate systems (for arrays) differ in that numbers increase from top-to-bottom:**\n",
    "\n",
    "![](assets/coordinate-system-diagram.png)\n",
    "\n",
    "When working with a coordinate system that uses latitude, that means that the vertical coordinates go from the most southern (negative) latitude to the most northern (positive) latitude. **Essentially, are image is flipped upside-down because the most negative coordinates are at the top of the image.** We can easily flip an array right-side up using `np.flipud()` (\"flip upside-down\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dd59c-b288-41c3-818c-2039cda5c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(np.flipud(et[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087afd3f-99d7-45f0-8454-518ca4eb479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note data type, why we're changing it to an array\n",
    "\n",
    "type(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421aff6-d5a5-41ed-8e2a-58000e89776f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Opening multiple files with `xarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a0d0f-ef43-4531-9b05-8f73444fa61b",
   "metadata": {},
   "source": [
    "Now, let's switch back to `xarray`.\n",
    "\n",
    "**Again, we have several files representing different points in time.** Instead of writing a `for` loop, this time we'll use `open_mfdataset()` (\"open multi-file dataset\") to collect all the files into a single dataset. \n",
    "\n",
    "The string `'data_raw/NLDAS/*.nc'` describes where the files we want to open are located, where `*` is a wildcard: a symbol matching any number of text characters that may be present in a filename. In this case, we want to open *all* the netCDF files (`*.nc`) in the `data_raw/NLDAS` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f86436-943d-4e04-afb4-eef26b55efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all the files as a single xarray Dataset\n",
    "ds = xr.open_mfdataset('data_raw/NLDAS/*.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4da220-aaf2-4eac-8590-db8ff4fbb46f",
   "metadata": {},
   "source": [
    "**These NLDAS files contain multiple different variables...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9269f-b60c-4e56-8dbd-f60b5a8c24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14855d40-79e6-4aa2-872e-76c4ec5afff7",
   "metadata": {},
   "source": [
    "**In this case study, we're primarily interested in the variables that quantify the state of the water cycle or evaporative stress:**\n",
    "\n",
    "- `Evap`: This is total evapotranspiration\n",
    "- `SWdown`: Down-welling short-wave radiation, i.e., the amount of solar radiation directed downwards\n",
    "- `SMAvail_0_100cm`: The total liquid water in the top 100 cm of soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94adc4-98f7-4f1f-99f7-64043d40aac3",
   "metadata": {},
   "source": [
    "Now we've seen one of the big advantages of the `xarray` library: `xarray` already knows how netCDF variables should be displayed. It is capable of figuring out, based on the coordinates, how the image should be oriented.\n",
    "\n",
    "Below, because our dataset, `ds`, has more than one time step, we use the notation `ds['Evap'][0]` to subset the data to the first (zeroth) time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec11f90-a768-41e4-85ed-016a32382805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image in the time series\n",
    "ds['Evap'][0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ba162-104c-4de3-ac8d-7f17cdc318af",
   "metadata": {},
   "source": [
    "**However, if we extract a netCDF variable as a NumPy array, it will still be upside-down:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e87ed1-de0d-4f63-b420-a9b11dc2a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = ds['Evap'].to_numpy()\n",
    "\n",
    "pyplot.imshow(array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a60e1-9815-42ef-b1d3-d0cbf7d5ac92",
   "metadata": {},
   "source": [
    "This is why we must be careful when working with netCDF data, regardless of whether we use the `xarray` or `netCDF4` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094b6e3-21c6-43a2-8257-6d998d4843c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing a climatology\n",
    "\n",
    "**What distinguishes a flash drought or any drought from non-drought conditions?** We know that drought is characterized by reduced precipitation, reduced soil moisture, or both, but what is the magnitude of the reduction? To answer this question, we'd need to compare \"drought conditions\" to \"average conditions.\" That is, compared to a *long-term average,* what is the magnitude of the change in a meteorological condition, like monthly precipitation?\n",
    "\n",
    "The *long-term average* of a climate variable, calculated for some recurring interval (days, months, years), is called a **climatology.** In this case study, we're interested in how severe the August 2017 drought conditions were. We could quantify that by computing an August evapotranspiration climatology, which is the average August evapotranspiration (ET) over a long period of record/\n",
    "\n",
    "In this case, we have 10 years of monthly August ET, as indicated by the first axis of our array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f516f-c651-4c2a-8a2e-77096275536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Evap'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77700e-4080-4276-9213-0afc7265493f",
   "metadata": {},
   "source": [
    "Computing a climatology, therefore, is as easy as calling `mean()` on our array and collapsing the first axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b5090-84d5-4dc7-9115-71f70d7d987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clim = ds['Evap'].mean(axis = 0)\n",
    "et_clim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2949fe-b779-4009-8b26-52e807693bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We're flipping the et_clim array upside-down because of the CF convention\n",
    "pyplot.imshow(np.flipud(et_clim))\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration [kg m-2]')\n",
    "pyplot.title('Mean August ET')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778d73f-1c62-45d4-99fa-650972f0a9c4",
   "metadata": {},
   "source": [
    "### How does September 2017 compare?\n",
    "\n",
    "To figure out how much lower September ET was in 2017, we want to subtract the climatology from the September 2017 ET, effectively removing the average ET and showing only deviations from (above or below) the mean.\n",
    "\n",
    "When we subtract the mean from a time series, the result is often called the **anomaly** (deviation from the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76515bd-32a7-4a9e-9335-43566d6e19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_2017_anomaly = ds['Evap'][-1] - et_clim\n",
    "\n",
    "pyplot.imshow(np.flipud(et_2017_anomaly), cmap = 'RdYlBu')\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration Anomaly [kg m-2]')\n",
    "pyplot.title('September 2017 ET Anomaly')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132ea5f-062f-4255-bdd0-0946953410cd",
   "metadata": {},
   "source": [
    "#### Using `cartopy`\n",
    "\n",
    "Once again, it can be helpful to use `cartopy` to see our data in context. In this example, we use `cartopy` built-in support for [data from Natural Earth](https://www.naturalearthdata.com/) to see the U.S. state boundaries on top of our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69040f-e17b-4168-bea9-b703014e1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [\n",
    "    ds['lon'].to_numpy().min(),\n",
    "    ds['lon'].to_numpy().max(),\n",
    "    ds['lat'].to_numpy().min(),\n",
    "    ds['lat'].to_numpy().max()\n",
    "]\n",
    "extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f587d-58fb-4e91-a9e4-4f895f00047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "shapename = 'admin_1_states_provinces_lakes'\n",
    "states_shp = shpreader.natural_earth(resolution = '110m', category = 'cultural', name = shapename)\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(1, 1, 1, projection = ccrs.PlateCarree())\n",
    "ax.imshow(np.flipud(et_2017_anomaly), extent = extent, cmap = 'RdYlBu')\n",
    "ax.add_geometries(shpreader.Reader(states_shp).geometries(), ccrs.PlateCarree(), facecolor = 'none')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4d38e-bc5a-46c4-8ba6-08bedd709515",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Saving our reproducible workflow\n",
    "\n",
    "So far, we've seen a lot of different steps in our analysis of the NLDAS data. What did we just do? \n",
    "\n",
    "1. We downloaded publicly available, raw data from the NLDAS re-analysis dataset.\n",
    "2. We opened each file and extracted a variable as an array, stacking all the arrays together in chronological order.\n",
    "3. We computed a monthly climatology for the month of August.\n",
    "4. We computed the anomalies in our variable's time series by subtracting the climatology.\n",
    "5. We plotted the result.\n",
    "\n",
    "\n",
    "### From Data to Results\n",
    "\n",
    "**How can we preserve and keep track of the steps we've taken?**\n",
    "\n",
    "Put another way, how can we turn our processing steps into a **reproducible workflow?** One that can be saved, used again, and used by someone else to get the same result?\n",
    "\n",
    "In Open Science, we like to distinguish between **replication** and **reproducibility.** These terms are sometimes used interchangeably, which can be confusing. We use the definitions introduced by the National Science Foundation and reviewed by Goodman et al. (2016):\n",
    "\n",
    "- **Replication** of a scientific result involves the generation of *new evidence* to evaluate an existing scientific claim using the *same methods* as previous authors but with *different data,* likely newly collected. A scientific claim is \"replicable\" (or \"repeatable\") if a third party can perform the same experiment, using the same methods and protocols, but with different data and derive the same conclusion.\n",
    "- **Reproducibility** sets a lower, but still important bar, and \"it is important to note that a study that is reproducible is not necessarily repeatable\" (Cassey & Blackburn 2006). A claim or analysis \"is reproducible if the analytic data sets and the computer code used to create the data analysis are made available to others\" (Peng & Hicks 2021) and \"running the same software on the same input data [generates] the same results\" (Rougier et al. 2017).\n",
    "\n",
    "**Today, we're aiming for reproducibility.** We want to be able to give our Python code to someone else, point them to the publicly-available data we used, and allow them to generate the same result that we did.\n",
    "\n",
    "### Reuseable Code\n",
    "\n",
    "A simple way to represent one of our processing steps is as a reuseable Python function. Here's an example of what we might write to represent Step 2 (from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e0eac-8a7d-46d9-83d3-0c98b982ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_time_series(netcdf_file_list, variable, nodata = -9999):\n",
    "    '''\n",
    "    Generates a time series for a given variable, based on an \n",
    "    ordered list of netCDF4 files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    netcdf_file_list : list\n",
    "        The list of netCDF4 files, where each file represents a date\n",
    "    variable : str\n",
    "        The name of the variable of interest\n",
    "    nodata : int or float\n",
    "        The NoData value, which will be replaced with `np.nan`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    '''\n",
    "    series = []\n",
    "    for filename in file_list:\n",
    "        ds = xr.open_dataset(filename)\n",
    "        array = ds[variable].to_numpy()\n",
    "        # Don't forget to to flip the image upside-down!\n",
    "        series.append(np.flipud(array[0]))\n",
    "    \n",
    "    series = np.stack(series, axis = 0)\n",
    "    # Fill in the NoData values\n",
    "    series[series == nodata] = np.nan\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342c2a6-103a-4b1b-bd46-fa90120db52e",
   "metadata": {},
   "source": [
    "**The multi-line Python string (beginning with three quote characters, `'''`) marks the beginning of a Python *docstring* or documentation string.** The docstring must immediately follow the first line of the function definition.\n",
    "\n",
    "The docstring is what users see when they call for `help()` on your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b106e-c301-4520-81c2-fde2852812c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(stack_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458657a9-55dd-4644-bda8-d5f5b70214fa",
   "metadata": {},
   "source": [
    "**A good docstring tells the user:**\n",
    "\n",
    "- The purpose of the function; in our example, the function \"Generates a time series for a given variable...\"\n",
    "- What input *parameters* (arguments) the function accepts.\n",
    "- What the *return value* of the function is.\n",
    "\n",
    "It might also include one or more example use cases. The format of our docstring's \"Parameters\" and \"Return\" value are based on a convention (\"numpydoc\") and [you can read about that convention and alternatives at this reference.](https://pdoc3.github.io/pdoc/doc/pdoc/#supported-docstring-formats) Under the \"Parameters\" heading, we indicate the name of an input parameter, its type(s), and a brief explanation of what it means:\n",
    "\n",
    "```\n",
    "Parameters\n",
    "----------\n",
    "param_name : type\n",
    "    Indented 4 spaces, we describe the input parameter on the next line\n",
    "```\n",
    "\n",
    "The \"Returns\" heading is formatted in a similar way, except the return value doesn't have a name:\n",
    "\n",
    "```\n",
    "Returns\n",
    "-------\n",
    "type\n",
    "    Indented 4 spaces, we describe the output parameter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bbaffa-d210-4881-84d7-c07d06d365c4",
   "metadata": {},
   "source": [
    "### Using Our Python Function\n",
    "\n",
    "Most importantly, does our Python function work the way we expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf0a13-788e-43d9-8968-acf773a1d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "et = stack_time_series(file_list, 'Evap')\n",
    "et.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c131d1e-8a9a-4165-afa4-80e4bdbe7089",
   "metadata": {},
   "source": [
    "Recall that when we calculated the climatology (Step 3), the result was a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69ab3b-f7b5-40ce-a37a-202976043b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "et.mean(axis = 0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efe417-e6fb-4251-80c0-2ee2d1c018e2",
   "metadata": {},
   "source": [
    "But, when we're ready to calculate the anomaly, note that we subtract this 2D array from a 3D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85f51a-516f-4f69-9c93-ad1a7b7846b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(et.shape)\n",
    "print(et.mean(axis = 0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc47a3e-59b5-4ec9-a189-c829bf55591f",
   "metadata": {},
   "source": [
    "We might expect this not to work, since the two arrays have different shapes. However, as the last two axes are the same for both arrays, NumPy is able to figure out what we want to do when we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285dd31-d7a3-4a3e-8be8-f6839de17efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = et - et.mean(axis = 0)\n",
    "anomaly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9e30c-8f4e-4ffb-97ff-e625180ef154",
   "metadata": {},
   "source": [
    "**What NumPy has done automatically for us is called *broadcasting.*** NumPy can see that the 2D array is similar to the 3D array in all but the first axis, so it *broadcasts* the 2D array to span the 3D array. In this case, it's like copying the 2D array 10 times to match the 10-element axis of the 3D array.\n",
    "\n",
    "**Let's write a Python function to represent this step (Step 3), calculating the anomalies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94718bdc-ba67-4a22-b8c3-d106a3d0f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor: Write docstring together with learners\n",
    "\n",
    "def anomalies(time_series):\n",
    "    '''\n",
    "    Computes the anomaly (current value minus mean value) in a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series : numpy.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    '''\n",
    "    clim = time_series.mean(axis = 0)\n",
    "    return time_series - clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2deda0-8988-45a2-89f5-4b92acfd0863",
   "metadata": {},
   "source": [
    "### Putting it All Together\n",
    "\n",
    "Once we've written reuseable Python functions like this, we can begin to build a reproducible pipeline through **function composition,** i.e., chaining function calls together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2153-a470-4353-a460-8ebe3cbacb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('data_raw/*.nc')\n",
    "file_list.sort()\n",
    "\n",
    "et = stack_time_series(file_list, 'Evap')\n",
    "et_anomaly = anomalies(et)\n",
    "\n",
    "rad_anomaly = anomalies(stack_time_series(file_list, 'SWdown'))\n",
    "sm_anomaly = anomalies(stack_time_series(file_list, 'SMAvail_0_100cm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3e8e3-b3cb-4583-97d1-e326a3c9288b",
   "metadata": {},
   "source": [
    "**Notice that reproducibility automatically helps us to scale-up our analysis, as well.** The same functions we used for calculating an ET anomaly can be used to calculate an anomaly for any variable we're interested in! What do the anomalies in solar radiation (`\"SWdown\"`) and soil moisture (`\"SMAvail_0_100cm\"`) look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be24e1-4f85-4155-bc9a-9b59a1a071ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    et_anomaly[-1],\n",
    "    rad_anomaly[-1],\n",
    "    sm_anomaly[-1]\n",
    "]\n",
    "labels = ['ET', 'Radiation', 'Soil Moisture']\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 5))\n",
    "ax = fig.subplots(1, 3)\n",
    "for i in range(3):\n",
    "    ax[i].imshow(images[i], cmap = 'RdYlBu')\n",
    "    ax[i].set_title(labels[i] + ' Anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762ecbd-552c-4892-a296-2292906776a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bringing in NASA Earth Observations\n",
    "\n",
    "The NLDAS data we've used are a great tool for retrospective studies but, as a re-analysis dataset, it has some limitations:\n",
    "\n",
    "- It has a relatively high latency; it may be days or weeks before data are available.\n",
    "- It integrates data from multiple sources but with varying levels of accuracy and geographic coverage.\n",
    "\n",
    "If we want to characterize flash drought or detect it in near-real time, we shouldn't use re-analysis datasets. Instead, we want some kind of direct observation of drought conditions. **Let's see what we can learn about the 2017 Flash Drought from NASA's satellite-based soil-moisture estimates.**\n",
    "\n",
    "**We'll use data from NASA's Soil Moisture Active Passive (SMAP) Mission.** [NASA's earth observing missions provide data that is grouped into different processing levels:](https://www.earthdata.nasa.gov/engage/open-data-services-and-software/data-information-policy/data-levels)\n",
    "\n",
    "- **Level 1 (Raw data):** Basically, these are data values measured directly by a satellite instrument. They may or may not be physically interpretable. Most end-users won't benefit from Level 1 data.\n",
    "- **Level 2:** These are physically interpretable values that have been derived from the raw data, at the same spatial and temporal resolution as the Level 1 data. Level 2 data may be hard to use because the spatial structure of the data matches the instrument's viewing geometry.\n",
    "- **Level 3:** At Level 3, the geophysical values have been standardized on a uniform spatial grid and uniform time series. While some values may be missing due to low quality, clouds, or sensor failure, gridded Level 3 data from different time steps can be easily combined and compared.\n",
    "- **Level 4 (Model-enhanced data):** At Level 4, the values from Level 3 data are incorporated into some kind of model, possibly combining additional, independent datasets from other sensors in order to produce enhanced estimates or analyses of geophysical variables.\n",
    "\n",
    "### Downloading and Documenting the Data\n",
    "\n",
    "**[We'll use the 36-km Level 3 surface soil moisture data from the SMAP mission](https://nsidc.org/data/spl3smp/versions/8)** because these are a good compromise between direct sensor observations and ease of use.\n",
    "\n",
    "- At the website above, we can see there are multiple ways of accessing the data. [Let's use Earthdata Search;](https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008) can we access the data from NASA's cloud using `earthaccess`?\n",
    "- You may have noticed that the Level 3 SMAP data we want to use are *not* \"Available in Earthdata Cloud.\" It looks like we'll have to download the data directly.\n",
    "- **Where will we put the raw data we download?** Let's revisit our file tree in Jupyter Notebook.\n",
    "- **Within the `data_raw` folder, let's create a new folder called `SMAP_L3`.** This is where we'll put the data we're about to download.\n",
    "\n",
    "We've discussed the importance of having a well-documented workflow that makes it easy to understand how we obtained a particular scientific result. We assume that we can re-download the raw data we used anytime, but what if we forget where the data came from? Since the SMAP Level 3 data aren't available in the Cloud, we're about to do download the data manually, and it would be a good idea to document what steps we took to do that, in case there are questions about where the data came from or what kind of processing was applied.\n",
    "\n",
    "- In the Jupyter Notebook file tree, within the `SMAP_L3` folder let's make a new `\"New File\"`. Name the new text file `README.txt`.\n",
    "- Double-click `README.txt` to open it. This is where we'll add some useful information about the data we're about to download. Below is an example.\n",
    "\n",
    "```\n",
    "Author: K. Arthur Endsley\n",
    "Date: November 1, 2023\n",
    "\n",
    "This folder contains Level 3 data from the SMAP Mission. It was downloaded from:\n",
    "\n",
    "    https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008\n",
    "\n",
    "Here's some more information about this product:\n",
    "\n",
    "    https://nsidc.org/data/spl3smp/versions/8\n",
    "```\n",
    "\n",
    "This might not seem like a lot of information but there's plenty here that we would want to know if we took a long break from this project or if someone else had to try and figure out what we were doing. And its short length is also an advantage: **documenting your project doesn't have to be hard and any amount of information is better than none.**\n",
    "\n",
    "### Customizing an Earthdata Search Download\n",
    "\n",
    "The SMAP satellite has two overpasses every day, a \"morning\" and an \"afternoon\" overpass (local time). Let's use soil moisture data from the afternoon (PM) overpass, because this is likely when soil moisture stress on vegetation is at its peak.\n",
    "\n",
    "- [**This link will get you to the right place to start.**](https://search.earthdata.nasa.gov/search?q=SPL3SMP%20V008) Click on the one dataset that is shown on the right-hand side of the search window.\n",
    "- We'll download data from August and September to study the onset and progression of the 2017 Flash Drought: **Choose a temporal subset, 2017-06-01 through 2017-09-30.**\n",
    "- At the bottom right, **click the big green button that reads \"Download All.\"**\n",
    "- 3.8 GB is a lot of data! Can we make this download any smaller? We're only interested in soil moisture from the afternoon overpass. **Click \"Edit Options\" and under \"Select a data access method,\" select the \"Customize\" option.**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order.png)\n",
    "\n",
    "- **Scroll down to \"Configure data customization options\" and down to \"Band subsetting.\"**\n",
    "- **Within the text box that reads \"Filter\" type `soil_moisture_dca_pm`.** This will filter the available variables (\"bands\") to just this specific variable, which is the soil moisture estimate from the Dual-Channel Algorithm (DCA) for the afternoon (PM) overpass.\n",
    "- To make sure that `soil_moisture_dca_pm` is the *only* variable we download, **you'll need to uncheck the box next to `SPL3SMP` then re-check the box next to `soil_moisture_dca_pm` (see screenshot below).**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order_variables.png)\n",
    "  \n",
    "- Hit \"Done\" at the bottom of this form then the big green button that reads \"Download Data\"!\n",
    "\n",
    "#### But Wait!\n",
    "\n",
    "Because we selected a subset of variables, we'll have to wait to get an e-mail that the order is ready. **You don't need to do these steps yourself, because I already prepared all the data granules that would be downloaded this way.** They can be download directly from here:\n",
    "\n",
    "- [SMAP_L3_SPL3SMP_V008_20170601_20170930.zip](http://files.ntsg.umt.edu/data/ScienceCore/SMAP_L3_SPL3SMP_V008_20170601_20170930.zip) (Extract this ZIP file's contents to your `data_raw/SMAP_L3` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fcfd0-56f9-4ad7-9998-4104e4680432",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Reading SMAP Level 3 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae66ba-1f3f-4ead-a899-b68503ae252d",
   "metadata": {},
   "source": [
    "The SMAP Level 3 data we downloaded are each stored as a **Hierarchical Data File, version 5 (HDF5).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af777f84-05bf-4a48-b6d7-11ee827faf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "hdf = h5py.File('data_raw/SMAP_L3/SMAP_L3_SM_P_20170901_R18290_001_HEGOUT.h5', 'r')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc21c83-44dc-4ef4-a7a6-dad557c0241c",
   "metadata": {},
   "source": [
    "An HDF5 file is a lot like a netCDF4 file: they are both hierarhical files capable of storing multiple, diverse datasets and metadata in a single file. What do we mean by \"hierarchical\"? Well, an HDF5 or netCDF4 file is like a file tree, where *datasets* can be organized into different nested *groups,* as depicted below. Metadata, in the form of *attributes,* can be attached to any dataset or group throughout the file.\n",
    "\n",
    "![](assets/hdf5-structure.jpg)\n",
    "\n",
    "*Image courtesy of NEON Science.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f478567-664d-48cf-990a-f38ad78fa1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73580d-bc31-4f59-8064-ccce0d0ef783",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9469661-4435-4472-b429-ccee11945eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cad114-a67c-45d4-8941-e5914205bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Significance of an empty group?\n",
    "hdf['Metadata/ProcessStep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc84ca9-828b-411c-ad3b-4330e30fa509",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66735587-3c8e-4572-a004-338c00222ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs['softwareTitle']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573327d-a61f-45fe-8b82-9c2598767731",
   "metadata": {},
   "source": [
    "### Reading HDF5 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23df3c-e8c9-4f72-8f6d-09b53342d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_raw/SMAP_L3/SMAP_L3_SM_P_20170901_R18290_001_HEGOUT.h5'\n",
    "\n",
    "hdf = h5py.File(filename, 'r')\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281dc17-0562-4902-b878-ba86687911fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Remark on importance of closing a file\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4937b54-50e1-43de-b426-23745f86caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Explain that we'd like to open the file with xarray\n",
    "# TODO Note there are no coordinates\n",
    "\n",
    "ds = xr.open_dataset(filename, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf531a-2dab-4067-8db3-5bdde91b4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note coordinates assignment\n",
    "\n",
    "ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa77a5-e5e3-4a5d-95ba-8c6fbd7a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Remark on striping\n",
    "\n",
    "pyplot.figure(figsize = (12, 5))\n",
    "ds['soil_moisture_dca_pm'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb921e8-3f9e-48a1-9539-227b5a0e616e",
   "metadata": {},
   "source": [
    "### Challenge: Write a Function to Process SMAP L3 Data\n",
    "\n",
    "Based on what we just did above, write a single function that:\n",
    "\n",
    "- Accepts a file path to a SMAP L3 `*.h5` file, as a Python string\n",
    "- Returns an `xr.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620a0f9-6c71-4d88-add7-44a86e73b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smap_l3(file_path):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The file path to the SMAP L3 file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "    '''\n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "        return ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097031-2c99-4c26-a433-2bd7ef4a6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyl4c.ease2 import ease2_from_wgs84\n",
    "\n",
    "help(ease2_from_wgs84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8b08d-3a89-4161-bd3f-687d5f6010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the upper-left corner coordinates\n",
    "upper_left = ease2_from_wgs84((-109, 49), grid = 'M36')\n",
    "upper_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c193cd3-a6c5-4438-8093-c1cb3658b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_right = ease2_from_wgs84((-95, 43), grid = 'M36')\n",
    "lower_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780896-f79e-493c-bbfa-99c049824b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_smap_l3('data_raw/SMAP_L3/SMAP_L3_SM_P_20170801_R18290_001_HEGOUT.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743b113-52db-4644-8abc-7874d3cd662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['soil_moisture_dca_pm'][49:59,187:227].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6c111-b71c-4cde-b474-3f72d528138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['soil_moisture_dca_pm'][49:59,187:227].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83708eb4-2559-42a4-889a-55d34041ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note the importance of sorting the files!\n",
    "\n",
    "file_list = glob.glob('data_raw/SMAP_L3/*.h5')\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e44bd-acb6-4622-ba67-97861f278ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list.sort()\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74b5c0-d902-497a-8207-a7dcd69aa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_left = ease2_from_wgs84((-109, 49), grid = 'M36')\n",
    "lower_right = ease2_from_wgs84((-97, 43), grid = 'M36')\n",
    "r0, c0 = upper_left\n",
    "r1, c1 = lower_right\n",
    "\n",
    "sm_mean = []\n",
    "for filename in file_list:\n",
    "    ds = process_smap_l3(filename)\n",
    "    sm_mean.append(ds['soil_moisture_dca_pm'][r0:r1,c0:c1].mean().values)\n",
    "\n",
    "sm_mean = np.hstack(sm_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de213d90-68ef-41e5-af1a-6107579488e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pandas.date_range('2017-06-01', '2017-09-30', freq = '1D')\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d620c6-5f8c-43e7-b767-2e0a68d32bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Talk about: 1) The gaps; 2) The oscillations; 3) The increase in soil moisture after Sept. 15\n",
    "\n",
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.plot(dates, sm_mean, 'k-')\n",
    "pyplot.ylabel('Volumetric Soil Moisture (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e6f4e-eff7-42e4-8578-ea8363a2d525",
   "metadata": {},
   "source": [
    "### Calculating a Moving Average\n",
    "\n",
    "One way to address the gaps might be to calculate a moving average, filling in missing values for a given date with the average of the values from adjacent dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44023e3-5698-4dc0-8fd5-ce40f260a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = []\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    # Skip the first and last files\n",
    "    if i == 0 or i == (len(file_list) - 1):\n",
    "        continue\n",
    "\n",
    "    # For the previous, current, and next dates...\n",
    "    sm_stack = []\n",
    "    for j in [i-1, i, i+1]:\n",
    "        ds = process_smap_l3(file_list[j])\n",
    "        sm = ds['soil_moisture_dca_pm'][r0:r1,c0:c1]\n",
    "        sm_stack.append(sm)\n",
    "\n",
    "    # Take the average of the 3 values in each pixel, excluding NaNs\n",
    "    sm_stack = np.nanmean(np.stack(sm_stack, axis = 0), axis = 0)\n",
    "    # Then, compute the overall mean for the region of interest\n",
    "    time_series.append(sm_stack.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58063d-2bb6-4d33-a1e2-347bcc4ed1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc28bc4-22b1-41a1-bdb1-323e80551296",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(sm_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea374a-abb7-42ba-bd14-5b181c95c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.plot(dates[1:-1], time_series, 'k-')\n",
    "pyplot.ylabel('Volumetric Soil Moisture (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64bda8-8d1d-4ef1-87a9-5fed52fd8a56",
   "metadata": {},
   "source": [
    "### Summary: Reading HDF5 and netCDF4 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b387aa-ec14-4496-a012-8bceff9b602c",
   "metadata": {},
   "source": [
    "|                            |  HDF5                              | netCDF4                                | `xarray` (for both)        |\n",
    "|:---------------------------|:-----------------------------------|:---------------------------------------|:---------------------------|\n",
    "|Module import               | `import h5py`                      | `import netCDF4`                       | `import xarray as xr`      |\n",
    "|Files opened with...        | `hdf = h5py.File(...)`             | `nc = netCDF4.Dataset()`               | `ds = xr.open_dataset()`   |\n",
    "|Datasets/groups viewed...   | `hdf.keys()`                       | `nc.variables` or `nc.variables.keys()`| `list(ds.variables.keys())`|\n",
    "|                            | `hdf['group_name'].keys()`         | `nc.variables['group_name'].keys()`    |                            |\n",
    "|Datasets accessed through...| `hdf`                              | `nc.variables`                         | `ds.variables`             |\n",
    "|Attributes listed through...| `hdf.attrs`                        | `nc.ncattrs()`                         | `ds.attrs`                 |\n",
    "|                            | `hdf['dataset'].attrs`             | `nc.variables['dataset'].ncattrs()`    |                            |\n",
    "|Attributes read by...       | `hdf['dataset'].attrs['attribute']`| `nc.variables['dataset'].getncattr()`  | `ds.variables['dataset']`  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f04ad-61cd-40c5-8db1-a218161b48ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## More Resources\n",
    "\n",
    "- Curious about how to use `earthaccess.open()` along with `xarray` so that you don't have keep any downloaded files around? Well, `xarray.open_dataset()` can be slow when you have a lot of files to open, as in this time-series example. [This article describes how you can speed up `xarray.open_dataset()`](https://climate-cms.org/posts/2018-09-14-dask-era-interim.html) when working with multiple cloud-hosted files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ae193-7a05-4bde-8264-209483e74743",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Chen, L. G., J. Gottschalck, A. Hartman, D. Miskus, R. Tinker, and A. Artusa. 2019. Flash drought characteristics based on U.S. Drought Monitor. Atmosphere 10 (9):498.\n",
    "- He, M., J. S. Kimball, Y. Yi, S. W. Running, K. Guan, K. Jensco, B. Maxwell, and M. Maneta. 2019. Impacts of the 2017 flash drought in the US Northern plains informed by satellite-based evapotranspiration and solar-induced fluorescence. Environmental Research Letters 14 (7):074019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
