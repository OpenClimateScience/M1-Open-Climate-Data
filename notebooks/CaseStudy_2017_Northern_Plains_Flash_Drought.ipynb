{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79bde19-ec9b-401e-8ad0-6bfad1ad6dc3",
   "metadata": {},
   "source": [
    "# M1.8 - Case Study: 2017 Northern Plains Flash Drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea7189-42c4-4f69-89c1-99053040b61b",
   "metadata": {},
   "source": [
    "Let's use everything we've learned so far to study a real drought event in an agricultural system.\n",
    "\n",
    "Whereas meteorological drought is characterized by a deficit of precipitation, **flash drought** is characterized by a sudden, extreme demand for water from the land surface: \"anomalously high evapotranspiration rates, caused by anomalously high temperatures, winds, and/or incoming radiation\" (Chen et al. 2019).\n",
    "\n",
    "In 2017, a flash drought emerged in the Northern Plains of the United States. The U.S. Drought Monitor estimated that, on September 5, 2017, about 23% of the region experienced \"extreme drought\" conditions and subsequent crop losses and water shortages (He et al. 2019, *Environmental Research Letters*).\n",
    "\n",
    "**For this case study, we're going to consider the following climate variables and data sources:**\n",
    "\n",
    "- Evapotranspiration, radiation, and soil moisture data from [the North American Land Data Assimilation System (NLDAS)](https://disc.gsfc.nasa.gov/datasets/NLDAS_NOAH0125_M_2.0/summary?keywords=NLDAS), a re-analysis dataset.\n",
    "- Air temperature, pressure, and humidity, from [the NLDAS forcing data](https://disc.gsfc.nasa.gov/datasets/NLDAS_FORA0125_M_2.0/summary?keywords=NLDAS)\n",
    "- Soil moisture the Soil Moisture Active Passive (SMAP) mission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c13a88-9869-4dad-8384-e863e384589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7b524-c0a6-46d7-9a8c-255d2a9a5c55",
   "metadata": {},
   "source": [
    "## Organizing our file system\n",
    "\n",
    "We'll need a place to store these raw data. It's important that we have a folder in our file system reserved for these raw data so we can keep them separate from any new datasets (outputs) we might create. \n",
    "\n",
    "**Create a folder called `outputs` in your Jupyter Notebook's file system.** You can do this from the Jupyter Notebook home page (the file tree) by selecting \"New\" and then \"New Folder\" as in the screenshot below.\n",
    "\n",
    "![](./assets/M1_screenshot_Jupyter_new_folder.png)\n",
    "\n",
    "**Let's also create a folder called `data_raw` in our Jupyter Notebook's file system. Create two sub-folders within `data_raw`:**\n",
    "\n",
    "- `NLDAS`\n",
    "- `SMAP_L3`\n",
    "\n",
    "We should never modify the raw data (that we're about to download). Doing so would make it hard to repeat the analysis we're going to perform as we will lose the original data values. This doesn't mean we have to keep the `data_raw` folder around forever: if it's publicly available data, we can always download it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22575e4c-dada-4ebe-8d8e-67b5f55fc39d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Downloading the NLDAS data\n",
    "\n",
    "We'll use the function `earthaccess.search_data()` again. In this case, the `short_name` and `version` can be found on [the Goddard Earth Sciences (GES) Data and Information Services Center (DISC) website for this product.](https://disc.gsfc.nasa.gov/datasets/NLDAS_NOAH0125_M_2.0/summary?keywords=NLDAS)\n",
    "\n",
    "**The NLDAS data we're interested in are compiled monthly and we want to download an August monthly dataset for each year.** Because the dates we want are non-consecutive, we need to call `earthaccess.search_data()` within a `for` loop. Below, we also use string formatting so that the string `f'{year}-08'` becomes, e.g.: `'2008-08'`, `'2009-08'`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cde0a-21a3-4a6d-bc78-78aa58b51804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor: Show how to find the \"short_name\" and \"version\"\n",
    "\n",
    "results = []\n",
    "\n",
    "# Get data from August for every year from 2008 up to (but not including) 2018\n",
    "for year in range(2008, 2018):\n",
    "    search = earthaccess.search_data(\n",
    "        short_name = 'NLDAS_NOAH0125_M',\n",
    "        version = '2.0',\n",
    "        temporal = (f'{year}-08', f'{year}-08'))\n",
    "    results.extend(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f8847-6347-4ad7-8179-de355a4ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12eab9d-a5d7-4eef-8191-140c30c037e8",
   "metadata": {},
   "source": [
    "Previously, we've used `earthaccess.open()` to get access to these data. This time, we'll use `earthaccess.download()`. What's the difference?\n",
    "\n",
    "- `earthaccess.open()` provides a file-like object that is available to be downloaded and read *only we need it.*\n",
    "- `earthaccess.download()` actually downloads the file to our file system.\n",
    "\n",
    "**Note that, below, we're telling `earthaccess.download()` to put the downloaded files into our new `data_raw` folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3692c6-ae6c-4f6d-b919-747917eb9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(results, 'data_raw/NLDAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcd7b2-9b50-4367-847e-dce4cae46fdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reading netCDF files without `xarray`\n",
    "\n",
    "Although we could open these netCDF files using `xarray`, we're going to first use a different Python library called `netCDF4`. This will help us learn more about the netCDF file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8743148-9ae6-434e-89bc-ee10f2e3296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "\n",
    "nc = netCDF4.Dataset('data_raw/NLDAS/NLDAS_NOAH0125_M.A200808.020.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0479a8-8cc0-4dee-bcf1-5003badaa8f1",
   "metadata": {},
   "source": [
    "As we previously discussed, one of the advantages of the netCDF file format is that it is **self-documenting;** there are file-level and dataset-level descriptive information, or metadata, called **attributes.**\n",
    "\n",
    "When using the `netCDF4` library to open a netCDF file, we can access **file-level attributes** this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47921a58-b40f-4fc9-8f57-99187054d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.ncattrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9202eb-b4f3-4772-a560-6ac83006ac7b",
   "metadata": {},
   "source": [
    "And we can access **dataset-level attributes** this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375fdb-7939-48c0-9b3c-76dde699858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc['Evap'].ncattrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf959b-7041-4f53-b031-f40ea60eb250",
   "metadata": {},
   "source": [
    "### Getting real values from netCDF datasets\n",
    "\n",
    "There are some significant differences between how the `xarray` and `netCDF4` libraries represent netCDF files. One important difference is how array data are read from the files.\n",
    "\n",
    "The `netCDF4` package reads array data from the file the way it is stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a08166-9af5-4359-a061-45df888bc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc['Evap']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66a146-d0ab-4674-b24f-2539a81f0a10",
   "metadata": {},
   "source": [
    "**Notice the `scale_factor`, `add_offset`, and `missing_value` attributes.** These are very important to consider because of the way netCDF4 files sometimes store variables. If the variables are packed in a certain way to save disk space, we need to transform the packed values into real values before using the data:\n",
    "\n",
    "$$\n",
    "\\text{Real value} = (\\text{Packed value}\\times \\text{Scale factor}) + \\text{Offset}\n",
    "$$\n",
    "\n",
    "**When we look at the `\"Evap\"` (total evapotranspiration) dataset's attributes with `xarray`, however...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfff4f-9a2a-486a-a89d-44a477612ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data_raw/NLDAS/NLDAS_NOAH0125_M.A200808.020.nc')\n",
    "\n",
    "ds['Evap'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb860609-7556-4dfd-868c-c353a791f81d",
   "metadata": {},
   "source": [
    "**Note that the attributes are different!** The `scale_factor`, `add_offset`, and `missing_value` attributes are missing.\n",
    "\n",
    "**This is because `xarray` transforms packed values into real values automatically for us.** Compare the two examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c1fa0-176f-4380-9209-ef5d3cbee4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(nc['Evap'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4e8c7-4d0b-4f3a-85d5-ff44bb342999",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ds['Evap'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0c869-d4ad-4c76-b7a6-cd0ad40e36e3",
   "metadata": {},
   "source": [
    "**In this case, the `scale_factor` is `1.0` and the `add_offset` is `0.0`, meaning the packed values are the same as the real values.** Hence, there is no difference in the numbers for valid data areas, above; but we can see that the `xarray.Dataset` replaced the `missing_value` (-9999) with `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3eea39-9872-42ce-be6e-958c471bee12",
   "metadata": {},
   "source": [
    "### Plotting netCDF4 Variables\n",
    "\n",
    "Let's plot the data from the first monthly dataset (August 2008). **Recall that, when using `pyplot.imshow()`, we have to provide a 2D array...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a17ca7-9a8c-4cb6-8068-55338d46db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "et = nc['Evap']\n",
    "\n",
    "et.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8d8b5-7d87-4464-93b4-69ce51a1f90b",
   "metadata": {},
   "source": [
    "The first axis of our `et` array is trivial, as it has only one element. We can simply subset the `et` array to this \"first\" (and only) element using `et[0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0e2bd-12db-432f-af8c-f684d98b73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(et[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f917ad-80e6-4c06-9fd1-904cd3bce7ee",
   "metadata": {},
   "source": [
    "Does this look right? Why is it upside down?\n",
    "\n",
    "The reason is because of [the CF Convention](http://cfconventions.org/) that defines how netCDF4 files should be formatted. Part of that standard requires that the coordinate arrays (here, latitude and longitude arrays) be sorted from smallest number to largest number. **Whereas spatial coordinate systems like latitude-longitude have numbers increasing from bottom-to-top and left-to-right, image coordinate systems (for arrays) differ in that numbers increase from top-to-bottom:**\n",
    "\n",
    "![](assets/coordinate-system-diagram.png)\n",
    "\n",
    "When working with a coordinate system that uses latitude, that means that the vertical coordinates go from the most southern (negative) latitude to the most northern (positive) latitude. **Essentially, are image is flipped upside-down because the most negative coordinates are at the top of the image.** We can easily flip an array right-side up using `np.flipud()` (\"flip upside-down\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dd59c-b288-41c3-818c-2039cda5c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(np.flipud(et[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087afd3f-99d7-45f0-8454-518ca4eb479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note data type, why we're changing it to an array\n",
    "\n",
    "type(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421aff6-d5a5-41ed-8e2a-58000e89776f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Opening multiple files with `xarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a0d0f-ef43-4531-9b05-8f73444fa61b",
   "metadata": {},
   "source": [
    "Now, let's switch back to `xarray`.\n",
    "\n",
    "**Again, we have several files representing different points in time.** Instead of writing a `for` loop, this time we'll use `open_mfdataset()` (\"open multi-file dataset\") to collect all the files into a single dataset. \n",
    "\n",
    "The string `'data_raw/NLDAS/*.nc'` describes where the files we want to open are located, where `*` is a wildcard: a symbol matching any number of text characters that may be present in a filename. In this case, we want to open *all* the netCDF files (`*.nc`) in the `data_raw/NLDAS` directory that contain the word `\"NOAH\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f86436-943d-4e04-afb4-eef26b55efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all the files as a single xarray Dataset\n",
    "ds = xr.open_mfdataset('data_raw/NLDAS/*NOAH*.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4da220-aaf2-4eac-8590-db8ff4fbb46f",
   "metadata": {},
   "source": [
    "**These NLDAS files contain multiple different variables...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9269f-b60c-4e56-8dbd-f60b5a8c24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14855d40-79e6-4aa2-872e-76c4ec5afff7",
   "metadata": {},
   "source": [
    "**In this case study, we're primarily interested in the variables that quantify the state of the water cycle or evaporative stress:**\n",
    "\n",
    "- `Evap`: This is total evapotranspiration\n",
    "- `SWdown`: Down-welling short-wave radiation, i.e., the amount of solar radiation directed downwards\n",
    "- `SMAvail_0_100cm`: The total liquid water in the top 100 cm of soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94adc4-98f7-4f1f-99f7-64043d40aac3",
   "metadata": {},
   "source": [
    "Now we've seen one of the big advantages of the `xarray` library: `xarray` already knows how netCDF variables should be displayed. It is capable of figuring out, based on the coordinates, how the image should be oriented.\n",
    "\n",
    "Below, because our dataset, `ds`, has more than one time step, we use the notation `ds['Evap'][0]` to subset the data to the first (zeroth) time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec11f90-a768-41e4-85ed-016a32382805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image in the time series\n",
    "ds['Evap'][0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ba162-104c-4de3-ac8d-7f17cdc318af",
   "metadata": {},
   "source": [
    "**However, if we extract a netCDF variable as a NumPy array, it will still be upside-down:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e87ed1-de0d-4f63-b420-a9b11dc2a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = ds['Evap'][:]\n",
    "\n",
    "pyplot.imshow(array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a60e1-9815-42ef-b1d3-d0cbf7d5ac92",
   "metadata": {},
   "source": [
    "This is why we must be careful when working with netCDF data, regardless of whether we use the `xarray` or `netCDF4` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094b6e3-21c6-43a2-8257-6d998d4843c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing a climatology\n",
    "\n",
    "**What distinguishes a flash drought or any drought from non-drought conditions?** We know that drought is characterized by reduced precipitation, reduced soil moisture, or both, but what is the magnitude of the reduction? To answer this question, we'd need to compare \"drought conditions\" to \"average conditions.\" That is, compared to a *long-term average,* what is the magnitude of the change in a meteorological condition, like monthly precipitation?\n",
    "\n",
    "The *long-term average* of a climate variable, calculated for some recurring interval (days, months, years), is called a **climatology.** In this case study, we're interested in how severe the August 2017 drought conditions were. We could quantify that by computing an August evapotranspiration climatology, which is the average August evapotranspiration (ET) over a long period of record/\n",
    "\n",
    "In this case, we have 10 years of monthly August ET, as indicated by the first axis of our array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f516f-c651-4c2a-8a2e-77096275536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Evap'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77700e-4080-4276-9213-0afc7265493f",
   "metadata": {},
   "source": [
    "Computing a climatology, therefore, is as easy as calling `mean()` on our array and collapsing the first axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b5090-84d5-4dc7-9115-71f70d7d987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clim = ds['Evap'].mean(axis = 0)\n",
    "et_clim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2949fe-b779-4009-8b26-52e807693bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We're flipping the et_clim array upside-down because of the CF convention\n",
    "pyplot.imshow(np.flipud(et_clim))\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration [kg m-2]')\n",
    "pyplot.title('Mean August ET')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778d73f-1c62-45d4-99fa-650972f0a9c4",
   "metadata": {},
   "source": [
    "### How does August 2017 compare?\n",
    "\n",
    "To figure out how much lower August ET was in 2017, we want to subtract the climatology from the August 2017 ET, effectively removing the average ET and showing only deviations from (above or below) the mean.\n",
    "\n",
    "When we subtract the mean from a time series, the result is often called the **anomaly** (deviation from the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76515bd-32a7-4a9e-9335-43566d6e19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_2017_anomaly = ds['Evap'][-1] - et_clim\n",
    "\n",
    "pyplot.imshow(np.flipud(et_2017_anomaly), cmap = 'RdYlBu')\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('Evapotranspiration Anomaly [kg m-2]')\n",
    "pyplot.title('August 2017 ET Anomaly')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fe09f-76f6-4969-85ce-ad1726c7ef5b",
   "metadata": {},
   "source": [
    "We can compute the anomalies in *every year* by simply writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592df46b-5667-4ebc-b015-2142b21afbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_anomaly = ds['Evap'] - et_clim\n",
    "et_anomaly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca8a6c-2d8b-4c4e-84cd-e3e702fcbdd6",
   "metadata": {},
   "source": [
    "Above, we subtracted a 2D array from a 3D array. This works because NumPy is capable of figuring out what mathematical operation you want to perform, even though the arrays have different shapes. NumPy compares each axis, starting with the trailing axis, and allows the smaller array to be *broadcast* to match the shape of the larger array. **Broadcasting** allows allows you to perform operations on arrays that are not otherwise compatible. Broadcasting helps us to write **vectorized code,** or code that works the same way regardless of the shapes of the input arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132ea5f-062f-4255-bdd0-0946953410cd",
   "metadata": {},
   "source": [
    "#### Using `cartopy`\n",
    "\n",
    "Once again, it can be helpful to use `cartopy` to see our data in context. In this example, we use `cartopy` built-in support for [data from Natural Earth](https://www.naturalearthdata.com/) to see the U.S. state boundaries on top of our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69040f-e17b-4168-bea9-b703014e1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [\n",
    "    ds['lon'].to_numpy().min(),\n",
    "    ds['lon'].to_numpy().max(),\n",
    "    ds['lat'].to_numpy().min(),\n",
    "    ds['lat'].to_numpy().max()\n",
    "]\n",
    "extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f587d-58fb-4e91-a9e4-4f895f00047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "shapename = 'admin_1_states_provinces_lakes'\n",
    "states_shp = shpreader.natural_earth(resolution = '110m', category = 'cultural', name = shapename)\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(1, 1, 1, projection = ccrs.PlateCarree())\n",
    "ax.imshow(np.flipud(et_2017_anomaly), extent = extent, cmap = 'RdYlBu')\n",
    "ax.add_geometries(shpreader.Reader(states_shp).geometries(), ccrs.PlateCarree(), facecolor = 'none')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d149745-03e3-414a-bfe6-bc57e1eb916e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Writing re-useable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bed11-92ea-4585-b7d3-1a17a7761296",
   "metadata": {},
   "source": [
    "It looks like August 2017 was characterized by anomalously low evapotranspiration (ET) rates in the Northern Plains. What about other climate variables?\n",
    "\n",
    "We did a lot of work to create the ET anomaly plot. Is there a way we can easily apply the same workflow to our other climate variables?\n",
    "\n",
    "**This is what we create Python functions for: to automate a task in a consistent way.** Let's create a function for our current workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50353d6-6bb7-49db-8253-927edee9c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomaly(data):\n",
    "    '''\n",
    "    Calculates the anomaly in a long-term time series dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xarray.DataArray\n",
    "        The time-series data, a (T x M x N) array where T is the \n",
    "        number of time steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        The anomaly values\n",
    "    '''\n",
    "    clim = data.mean(axis = 0)\n",
    "    # Create a sequence of 2D maps, one for each year\n",
    "    return data - clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f5ba98-72a4-49a1-b0a9-db12e0a7d736",
   "metadata": {},
   "source": [
    "**The multi-line Python string (beginning with three quote characters, `'''`) marks the beginning of a Python *docstring* or documentation string.** The docstring must immediately follow the first line of the function definition.\n",
    "\n",
    "The docstring is what users see when they call for `help()` on your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b106e-c301-4520-81c2-fde2852812c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(calculate_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276e275-fab0-487c-a446-59999e66a8ed",
   "metadata": {},
   "source": [
    "**A good docstring tells the user:**\n",
    "\n",
    "- The purpose of the function; in our example, the function \"Generates a time series for a given variable...\"\n",
    "- What input *parameters* (arguments) the function accepts.\n",
    "- What the *return value* of the function is.\n",
    "\n",
    "It might also include one or more example use cases. The format of our docstring's \"Parameters\" and \"Return\" value are based on a convention (\"numpydoc\") and [you can read about that convention and alternatives at this reference.](https://pdoc3.github.io/pdoc/doc/pdoc/#supported-docstring-formats) Under the \"Parameters\" heading, we indicate the name of an input parameter, its type(s), and a brief explanation of what it means:\n",
    "\n",
    "```\n",
    "Parameters\n",
    "----------\n",
    "param_name : type\n",
    "    Indented 4 spaces, we describe the input parameter on the next line\n",
    "```\n",
    "\n",
    "The \"Returns\" heading is formatted in a similar way, except the return value doesn't have a name:\n",
    "\n",
    "```\n",
    "Returns\n",
    "-------\n",
    "type\n",
    "    Indented 4 spaces, we describe the output parameter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3433a06-45a1-43df-bbc1-ac9ff13a1d2f",
   "metadata": {},
   "source": [
    "**Most importantly, does our Python function work the way we expect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce74ca-5462-4d6a-acbd-22eb7a2385c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_anomaly = calculate_anomaly(ds['Evap'])\n",
    "pyplot.imshow(np.flipud(et_anomaly[-1]), cmap = 'RdYlBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a82c5-5681-4358-b0e2-5fdeba96665f",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "**Once we've written re-useable Python functions like this, we can begin to scale-up our analysis in new ways!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2153-a470-4353-a460-8ebe3cbacb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_anomaly = calculate_anomaly(ds['SWdown'])\n",
    "sm_anomaly = calculate_anomaly(ds['SMAvail_0_100cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebf2c2-2e78-4875-91e7-2cc212a82ff2",
   "metadata": {},
   "source": [
    "The same functions we used for calculating an ET anomaly can be used to calculate an anomaly for any variable we're interested in! What do the anomalies in solar radiation (`\"SWdown\"`) and soil moisture (`\"SMAvail_0_100cm\"`) look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be24e1-4f85-4155-bc9a-9b59a1a071ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    et_anomaly[-1],\n",
    "    rad_anomaly[-1],\n",
    "    sm_anomaly[-1]\n",
    "]\n",
    "labels = ['ET', 'Radiation', 'Soil Moisture']\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 5))\n",
    "ax = fig.subplots(1, 3)\n",
    "for i in range(3):\n",
    "    ax[i].imshow(np.flipud(images[i]), cmap = 'RdYlBu')\n",
    "    ax[i].set_title(labels[i] + ' Anomaly')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53446672-191b-4156-a148-ac7c18a780d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Saving our results\n",
    "\n",
    "We've done a lot of interesting work with the NLDAS data. What if we wanted to save our results for someone else to use? What might they need to know about what we've done, and how could we communicate that?\n",
    "\n",
    "This is where a **self-documenting** file format like netCDF can help!\n",
    "\n",
    "Starting with our `xarray.DataArray`, let's add some **metadata** in the form of **attributes.** What are some things that people should know about these data? The measurement units are important, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf73486-360a-4704-a12c-c467dd9df397",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_anomaly.attrs['units'] = 'kg m-2'\n",
    "et_anomaly.attrs['name'] = 'Evapotranspiration anomaly, relative to 2008-2017 climatology'\n",
    "et_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1445e3fc-5c2e-4340-a426-8088444ae362",
   "metadata": {},
   "source": [
    "We're now ready to create our new `xarray.Dataset`!\n",
    "\n",
    "The `xr.Dataset()` constructor function takes at least two arguments:\n",
    "\n",
    "- The data variables, usually in the form a Python dictionary with key-value pairs representing the variable name (key) and the `DataArray` (value).\n",
    "- The coordinates of the `DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546cebe-d42b-4d06-8a94-b1ba5221ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = xr.Dataset({'et_anomaly': et_anomaly}, ds.coords)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c030a35-8fbc-4be8-b061-e8358fde2677",
   "metadata": {},
   "source": [
    "We can also add some **file-level attributes.** If we ever wanted to change anything about this dataset, we might want to know what Python script was used to create it in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a24861-55c2-4036-93f1-3d7212929eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.attrs['source_file'] = 'CaseStudy_2017_Northern_Plains_Flash_Drought.ipynb'\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d104e96-2d2f-4729-ba2f-d5b32507520e",
   "metadata": {},
   "source": [
    "**Finally, we're ready to write our file to disk! But what filename should we choose?** For derived outputs, we should pick something *meaningful* that tells us information about:\n",
    "\n",
    "- What kind of data the file contains\n",
    "- What spatial locations or time periods it pertains to\n",
    "- What source data were used\n",
    "\n",
    "We'll also make sure to put the file in our `outputs` folder so that we don't mistake it for raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70073e-7912-4fbc-a4c9-e7dfc81b2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.to_netcdf('outputs/NLDAS_ET_anomalies_2008-2017.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb09626-5186-4547-b1c5-74ed46ed24be",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Computing vapor pressure deficit\n",
    "\n",
    "One aspect of the climate system we haven't yet examined is **vapor pressure deficit (VPD),** which is a measure of how dry the air is. VPD tells us the amount of additional water (in terms of vapor pressure) that the air could hold at its current temperature. Under high VPD, the atmosphere can act as a drinking straw, drawing water away from the Earth's surface and from plants. Did anomalously high VPD play a role in the 2017 Northern Plains flash drought?\n",
    "\n",
    "VPD isn't available as a climate variable in the NLDAS re-analysis dataset, but we can compute it from the variables that are available. To do that, we'll need to download a slightly different NLDAS data collection, consisting of the atmospheric forcing data that drive the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181ac3e-a9d3-4cae-a903-a0244980fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# Get data from August for every year from 2008 up to (but not including) 2018\n",
    "for year in range(2008, 2018):\n",
    "    search = earthaccess.search_data(\n",
    "        short_name = 'NLDAS_FORA0125_M',\n",
    "        version = '2.0',\n",
    "        temporal = (f'{year}-08', f'{year}-08'))\n",
    "    results.extend(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec2fb8-810c-47ea-80dd-1410cd65dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(results, 'data_raw/NLDAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84897fb-4873-488f-9b69-6b45ab3f4370",
   "metadata": {},
   "source": [
    "Once again, we want to open multiple files as a single `xarray.Dataset` using `open_mfdataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6926bd1-2e59-4fb3-a06c-1b8ed8dbf3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset('data_raw/NLDAS/NLDAS_FORA*.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360342e9-b690-4fc9-8753-f59967348cf9",
   "metadata": {},
   "source": [
    "The NLDAS data we downloaded has three variables we're interested in:\n",
    "\n",
    "- `Tair`, the air temperature in degrees Kelvin\n",
    "- `Qair`, the specific humidity\n",
    "- `PSurf`, the near-surface air pressure in Pascals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eea2c68-f336-4a86-9358-d1ed98807686",
   "metadata": {},
   "source": [
    "### Challenge: Write a function to compute VPD\n",
    "\n",
    "VPD is defined as the difference between the saturation vapor pressure (SVP) and the actual vapor pressure (AVP). That is, it is the difference between how much water the air *could* hold at its current temperature and the actual amount of water it currently holds.\n",
    "$$\n",
    "\\text{VPD} = \\text{SVP} - \\text{AVP}\n",
    "$$\n",
    "\n",
    "The August-Roche-Magnus formula is a good approximation for SVP:\n",
    "\n",
    "$$\n",
    "\\text{SVP} = 610.94\\times \\text{exp}\\left(\n",
    "\\frac{17.625\\times T}{T + 243.04}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "And an approximation for AVP is given by Gates (1980, *Biophysical Ecology*):\n",
    "\n",
    "$$\n",
    "\\text{AVP} = \\frac{Q\\times P}{0.622 + (0.379\\times Q)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $T$ is the air temperature in degrees Kelvin\n",
    "- $Q$ is the specific humidity\n",
    "- $P$ is the air pressure in Pascals\n",
    "- VPD, SVP, and AVP are also in Pascals\n",
    "- $\\text{exp}$ refers to the exponential function and is available in NumPy as `np.exp()`\n",
    "\n",
    "**Write a Python function called `vpd()` to compute VPD.** When writing your function, remember:\n",
    "\n",
    "- Write an informative docstring!\n",
    "- Be sure to add inline comments to describe complex or potentially confusing code.\n",
    "- Consider what your variable names should be and how you might use them to communicate measurement units.\n",
    "- When you've finished, compare it to the one written below.\n",
    "\n",
    "**Hint:** You'll need to convert air temperature from degrees Kelvin to degrees Celsius by subtracting 273.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17942f63-42de-4f9f-87fb-ac66c07a8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vpd(temp_k, pressure_pa, s_humidity):\n",
    "    '''\n",
    "    Computes vapor pressure deficit (VPD).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    temp_c : xarray.DataArray\n",
    "        Air temperature in degrees Kelvin\n",
    "    pressure_pa : xarray.DataArray\n",
    "        Air pressure in Pascals\n",
    "    s_humidity : xarray.DataArray\n",
    "        Specific humidity (dimensionless)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        VPD in Pascals\n",
    "    '''\n",
    "    temp_c = temp_k - 273.15\n",
    "    # Saturation vapor pressure (Pa)\n",
    "    svp = 610.94 * np.exp((17.625 * temp_c) / (temp_c + 243.04))\n",
    "    # Actual vapor pressure (Pa)\n",
    "    avp = (s_humidity * pressure_pa) / (0.622 + (0.379 * s_humidity))\n",
    "    return svp - avp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67872e3-2180-465e-848c-f48bb18ec095",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that you've written a function to compute VPD, let's apply it to our NLDAS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54261fe6-0b9e-4b44-99d2-0b07205be586",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpd_series = vpd(ds['Tair'], ds['PSurf'], ds['Qair'])\n",
    "\n",
    "vpd_anomaly = calculate_anomaly(vpd_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4639e6-d3cd-47f2-abd7-0f06038b68cd",
   "metadata": {},
   "source": [
    "From the plot below, it appears that only part of the Northern Plains experienced above-average VPD in August 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8447780-7792-4189-97bc-07e665cfd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(np.flipud(vpd_anomaly[-1]), cmap = 'RdYlBu')\n",
    "cbar = pyplot.colorbar()\n",
    "cbar.set_label('VPD Anomaly (Pa)')\n",
    "pyplot.title('August 2017 VPD Anomaly')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762ecbd-552c-4892-a296-2292906776a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bringing in NASA Earth observations\n",
    "\n",
    "The NLDAS data we've used are a great tool for retrospective studies but, as a re-analysis dataset, it has some limitations:\n",
    "\n",
    "- It has a relatively high latency; it may be days or weeks before data are available.\n",
    "- It integrates data from multiple sources but with varying levels of accuracy and geographic coverage.\n",
    "\n",
    "If we want to characterize flash drought or detect it in near-real time, we shouldn't use re-analysis datasets. Instead, we want some kind of direct observation of drought conditions. **Let's see what we can learn about the 2017 Flash Drought from NASA's satellite-based soil-moisture estimates.**\n",
    "\n",
    "**We'll use data from NASA's Soil Moisture Active Passive (SMAP) Mission.** [NASA's earth observing missions provide data that is grouped into different processing levels:](https://www.earthdata.nasa.gov/engage/open-data-services-and-software/data-information-policy/data-levels)\n",
    "\n",
    "- **Level 1 (Raw data):** Basically, these are data values measured directly by a satellite instrument. They may or may not be physically interpretable. Most end-users won't benefit from Level 1 data.\n",
    "- **Level 2:** These are physically interpretable values that have been derived from the raw data, at the same spatial and temporal resolution as the Level 1 data. Level 2 data may be hard to use because the spatial structure of the data matches the instrument's viewing geometry.\n",
    "- **Level 3:** At Level 3, the geophysical values have been standardized on a uniform spatial grid and uniform time series. While some values may be missing due to low quality, clouds, or sensor failure, gridded Level 3 data from different time steps can be easily combined and compared.\n",
    "- **Level 4 (Model-enhanced data):** At Level 4, the values from Level 3 data are incorporated into some kind of model, possibly combining additional, independent datasets from other sensors in order to produce enhanced estimates or analyses of geophysical variables.\n",
    "\n",
    "### Downloading and documenting the data\n",
    "\n",
    "**[We'll use the 36-km Level 3 surface soil moisture data from the SMAP mission](https://nsidc.org/data/spl3smp/versions/8)** because these are a good compromise between direct sensor observations and ease of use.\n",
    "\n",
    "- At the website above, we can see there are multiple ways of accessing the data. [Let's use Earthdata Search;](https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008) can we access the data from NASA's cloud using `earthaccess`?\n",
    "- You may have noticed that the Level 3 SMAP data we want to use are *not* \"Available in Earthdata Cloud.\" It looks like we'll have to download the data directly.\n",
    "- **Where will we put the raw data we download?** Let's revisit our file tree in Jupyter Notebook.\n",
    "- **Within the `data_raw` folder, let's create a new folder called `SMAP_L3`.** This is where we'll put the data we're about to download.\n",
    "\n",
    "We've discussed the importance of having a well-documented workflow that makes it easy to understand how we obtained a particular scientific result. We assume that we can re-download the raw data we used anytime, but what if we forget where the data came from? Since the SMAP Level 3 data aren't available in the Cloud, we're about to do download the data manually, and it would be a good idea to document what steps we took to do that, in case there are questions about where the data came from or what kind of processing was applied.\n",
    "\n",
    "- In the Jupyter Notebook file tree, within the `SMAP_L3` folder let's make a new `\"New File\"`. Name the new text file `README.txt`.\n",
    "- Double-click `README.txt` to open it. This is where we'll add some useful information about the data we're about to download. Below is an example.\n",
    "\n",
    "```\n",
    "Author: K. Arthur Endsley\n",
    "Date: November 1, 2023\n",
    "\n",
    "This folder contains Level 3 data from the SMAP Mission. It was downloaded from:\n",
    "\n",
    "    https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008\n",
    "\n",
    "Here's some more information about this product:\n",
    "\n",
    "    https://nsidc.org/data/spl3smp/versions/8\n",
    "```\n",
    "\n",
    "This might not seem like a lot of information but there's plenty here that we would want to know if we took a long break from this project or if someone else had to try and figure out what we were doing. And its short length is also an advantage: **documenting your project doesn't have to be hard and any amount of information is better than none.**\n",
    "\n",
    "### Customizing an Earthdata Search download\n",
    "\n",
    "The SMAP satellite has two overpasses every day, a \"morning\" and an \"afternoon\" overpass (local time). Let's use soil moisture data from the afternoon (PM) overpass, because this is likely when soil moisture stress on vegetation is at its peak.\n",
    "\n",
    "- [**This link will get you to the right place to start.**](https://search.earthdata.nasa.gov/search?q=SPL3SMP%20V008) Click on the one dataset that is shown on the right-hand side of the search window.\n",
    "- We'll download data from August and September to study the onset and progression of the 2017 Flash Drought: **Choose a temporal subset, 2017-06-01 through 2017-09-30.**\n",
    "- At the bottom right, **click the big green button that reads \"Download All.\"**\n",
    "- 3.8 GB is a lot of data! Can we make this download any smaller? We're only interested in soil moisture from the afternoon overpass. **Click \"Edit Options\" and under \"Select a data access method,\" select the \"Customize\" option.**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order.png)\n",
    "\n",
    "- **Scroll down to \"Configure data customization options\" and down to \"Band subsetting.\"**\n",
    "- **Within the text box that reads \"Filter\" type `soil_moisture_dca_pm`.** This will filter the available variables (\"bands\") to just this specific variable, which is the soil moisture estimate from the Dual-Channel Algorithm (DCA) for the afternoon (PM) overpass.\n",
    "- To make sure that `soil_moisture_dca_pm` is the *only* variable we download, **you'll need to uncheck the box next to `SPL3SMP` then re-check the box next to `soil_moisture_dca_pm` (see screenshot below).**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order_variables.png)\n",
    "  \n",
    "- Hit \"Done\" at the bottom of this form then the big green button that reads \"Download Data\"!\n",
    "\n",
    "#### But Wait!\n",
    "\n",
    "Because we selected a subset of variables, we'll have to wait to get an e-mail that the order is ready. **You don't need to do these steps yourself, because I already prepared all the data granules that would be downloaded this way.** They can be download directly from here:\n",
    "\n",
    "- [SMAP_L3_SPL3SMP_V008_20170601_20170930.zip](http://files.ntsg.umt.edu/data/ScienceCore/SMAP_L3_SPL3SMP_V008_20170601_20170930.zip) (Extract this ZIP file's contents to your `data_raw/SMAP_L3` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fcfd0-56f9-4ad7-9998-4104e4680432",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Reading SMAP Level 3 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae66ba-1f3f-4ead-a899-b68503ae252d",
   "metadata": {},
   "source": [
    "The SMAP Level 3 data we downloaded are each stored as a **Hierarchical Data File, version 5 (HDF5).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af777f84-05bf-4a48-b6d7-11ee827faf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = 'data_raw/SMAP_L3/SMAP_L3_SM_P_20170901_R18290_001_HEGOUT.h5'\n",
    "hdf = h5py.File(filename, 'r')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc21c83-44dc-4ef4-a7a6-dad557c0241c",
   "metadata": {},
   "source": [
    "An HDF5 file is a lot like a netCDF4 file: they are both hierarhical files capable of storing multiple, diverse datasets and metadata in a single file. What do we mean by \"hierarchical\"? Well, an HDF5 or netCDF4 file is like a file tree, where *datasets* can be organized into different nested *groups,* as depicted below. Metadata, in the form of *attributes,* can be attached to any dataset or group throughout the file.\n",
    "\n",
    "![](assets/hdf5-structure.jpg)\n",
    "\n",
    "*Image courtesy of NEON Science.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b05df1-cebb-4fb9-b532-c41d542047c2",
   "metadata": {},
   "source": [
    "We can look at the groups and datasets that are at the highest level of this hierarchy by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f478567-664d-48cf-990a-f38ad78fa1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67bfe3-965a-41fd-9e0f-c3a798f759c2",
   "metadata": {},
   "source": [
    "The `h5py.File` object, `hdf`, is accessed like a Python dictionary. If we want to look at the `'Metadata'` group, for example, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73580d-bc31-4f59-8064-ccce0d0ef783",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682e275-cd4b-49b2-aecf-61712aacec7a",
   "metadata": {},
   "source": [
    "This isn't very informative, but every group and dataset in an `h5py.File` object also behaves like a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9469661-4435-4472-b429-ccee11945eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eedb81-b0af-4ecf-8cee-46a70ca60fcd",
   "metadata": {},
   "source": [
    "The `'Metadata'` group is an example of how we might store information in an HDF5 file other than multi-dimensional arrays.\n",
    "\n",
    "What is the significance of this empty group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cad114-a67c-45d4-8941-e5914205bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a939d-338a-4dbe-a761-cf7976d4d308",
   "metadata": {},
   "source": [
    "Just like netCDF files, every dataset in an HDF5 file can be labeled with attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc84ca9-828b-411c-ad3b-4330e30fa509",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66735587-3c8e-4572-a004-338c00222ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs['processor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573327d-a61f-45fe-8b82-9c2598767731",
   "metadata": {},
   "source": [
    "### Reading HDF5 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfa9d8-3973-4632-9b86-8e5471e02dfe",
   "metadata": {},
   "source": [
    "We open an HDF5 file for reading with the `'r'` flag, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23df3c-e8c9-4f72-8f6d-09b53342d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = h5py.File(filename, 'r')\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed4753-4aef-4a43-9c3e-0a847cbdc8e4",
   "metadata": {},
   "source": [
    "Whenever we're finished working with an open HDF5 file, we should make sure to close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281dc17-0562-4902-b878-ba86687911fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cf24a-8164-49e9-aac7-d155880a3bcc",
   "metadata": {},
   "source": [
    "**Let's see what is different about opening the same file using `xarray`.** In particular, look at the **Data variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd13e73-0f04-4ab1-9a15-ad5dcc0fa21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filename)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81ced6-0b65-48b4-8290-04fada92528c",
   "metadata": {},
   "source": [
    "The single variable that was found, `\"crs\"`, is not going to be very useful to us.\n",
    "\n",
    "`xarray` has limitations when opening HDF5 files; it isn't able to determine what groups are available. Instead, we have to specify the group we want to open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4937b54-50e1-43de-b426-23745f86caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filename, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b6adc-f6ab-4ee8-8ac6-5503f7b9ec6b",
   "metadata": {},
   "source": [
    "Now we have a useful variable, `\"soil_moisture_dca_pm\"`, but our `xarray.Dataset` has no coordinates!\n",
    "\n",
    "One way to fix this would be to assign coordinates to our `xarray.Dataset`. This is why we need the `h5py` library, which is specialized for handling HDF5 files. We can read the `\"x\"` and `\"y\"` coordinates from our `h5py.File` and write them to the `xarray.Dataset`, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf531a-2dab-4067-8db3-5bdde91b4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note coordinates assignment\n",
    "\n",
    "hdf = h5py.File(filename, 'r')\n",
    "ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
    "hdf.close()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad3ae9-9d89-4266-8c56-dcb28bee3054",
   "metadata": {},
   "source": [
    "Now that we have both a **data variable** and **coordinates,** we're ready to plot the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa77a5-e5e3-4a5d-95ba-8c6fbd7a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (12, 5))\n",
    "ds['soil_moisture_dca_pm'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410de18-9d8f-40df-98ec-e8ac2452c6d9",
   "metadata": {},
   "source": [
    "**Notice the striping in this image.** The SMAP satellite has a revisit time of between 2 and 3 days. This means that, on a single day, the satellite's radiometer only images part of the globe. We could combine the morning, `\"soil_moisture_dca_am\"`, and afternoon, `\"soil_moisture_dca_pm\"`, overpasses for a single day, but soil moisture in many regions of the world varies quite a lot between morning and afternoon, so this might not be reasonable.\n",
    "\n",
    "We chose the `\"soil_moisture_dca_pm\"` (afternoon) overpass because the afternoon is typically when soil moisture stress is highest in terrestrial ecosystems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb921e8-3f9e-48a1-9539-227b5a0e616e",
   "metadata": {},
   "source": [
    "### Challenge: Write a Function to Process SMAP L3 Data\n",
    "\n",
    "Based on what we just did above, write a single function called `process_smap_l3` that:\n",
    "\n",
    "- Accepts a file path to a SMAP L3 `*.h5` file, as a Python string\n",
    "- Returns an `xr.Dataset`\n",
    "\n",
    "When you've finished, compare it to the one written below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620a0f9-6c71-4d88-add7-44a86e73b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smap_l3(file_path):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The file path to the SMAP L3 file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "    '''\n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "        return ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee212fd6-14f6-429c-b4d5-7893179250e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Subsetting the SMAP L3 data\n",
    "\n",
    "The SMAP soil moisture data are global but we're currently interested in a small study region, the Northern Plains of the U.S. How can we subset the SMAP data to our study region?\n",
    "\n",
    "You may have noticed that the coordinates we added to our `xarray.Dataset`, above, were not latitude-longitude coordinates. The SMAP data are projected onto an EASE-Grid 2.0, where \"EASE\" stands for Equal-Area Scalable Earth. This unique, global projection has many advantages but the X and Y coordinates can be hard to understand when we're used to working with latitude-longitude coordinates.\n",
    "\n",
    "**We'll use a tool from the `pyl4c` library to translate latitude-longitude (WGS84 datum) coordinates into the row-column coordinates of pixels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097031-2c99-4c26-a433-2bd7ef4a6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyl4c.ease2 import ease2_from_wgs84\n",
    "\n",
    "help(ease2_from_wgs84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc96f93-675b-4053-8b22-9c40a05c5356",
   "metadata": {},
   "source": [
    "Let's say that the upper-left corner of our study area is 49 degrees N latitude, 109 degrees W longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8b08d-3a89-4161-bd3f-687d5f6010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the upper-left corner coordinates\n",
    "upper_left = ease2_from_wgs84((-109, 49), grid = 'M36')\n",
    "upper_left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c4462-b57c-4114-a77c-0789c7f4a8c0",
   "metadata": {},
   "source": [
    "And the lower-right corner of our study area is 43 degrees N latitude, 95 degrees W longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c193cd3-a6c5-4438-8093-c1cb3658b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_right = ease2_from_wgs84((-95, 43), grid = 'M36')\n",
    "lower_right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68416bb9-4599-42bf-910f-51b369d4af72",
   "metadata": {},
   "source": [
    "Let's get an `xarray.Dataset` using the function we wrote and plot our study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780896-f79e-493c-bbfa-99c049824b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_smap_l3('data_raw/SMAP_L3/SMAP_L3_SM_P_20170801_R18290_001_HEGOUT.h5')\n",
    "\n",
    "ds['soil_moisture_dca_pm'][49:59,187:227].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f9dd8-0a75-493e-ad70-08423c41c152",
   "metadata": {},
   "source": [
    "**It's apparent that our study area was largely missed by the satellite on this particular data-day.** It's still possible to get a mean soil moisture value for the region..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6c111-b71c-4cde-b474-3f72d528138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['soil_moisture_dca_pm'][49:59,187:227].mean().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce3442-30a2-4f85-8239-d5d6d0f29d19",
   "metadata": {},
   "source": [
    "But this value may be biased because, depending on the day, it reflects the soil moisture conditions in different, smaller parts of our study region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1dc89-86d7-4aff-bc24-9f24b35b2202",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating a soil moisture time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e907b6e-9fdd-4099-9098-be798bd53e34",
   "metadata": {},
   "source": [
    "It'd be nice if we could use `xr.open_mf_dataset()` to open all these SMAP HDF5 files as a single time-series dataset. If we tried that, however, we'd find that it doesn't work because `xarray` doesn't know what the coordinates of an HDF5 dataset are, so it can't combine the datasets together.\n",
    "\n",
    "```python\n",
    "ds = xr.open_mfdataset('data_raw/SMAP_L3/*.h5', group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "```\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[166], line 1\n",
    "----> 1 ds = xr.open_mfdataset('data_raw/SMAP_L3/*.h5', group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "\n",
    "...\n",
    "\n",
    "ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8915f6-1620-42f8-ac12-0073d8c722bf",
   "metadata": {},
   "source": [
    "**This means we'll have to open each file ourselves and stack the arrays together.**\n",
    "\n",
    "We can use the `glob` library to get a list of all the files we want. The notation below, `'data_raw/SMAP_L3/*.h5'`, is similar to what we used with `xr.open_mfdataset()`: we're saying we want to use all the HDF5 files in a particular directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83708eb4-2559-42a4-889a-55d34041ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('data_raw/SMAP_L3/*.h5')\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909bd02-cb62-4509-9dfa-d3d099d5d8bd",
   "metadata": {},
   "source": [
    "**When we use `glob.glob()` for files that represent a time series, it's very important that we make sure the files are listed in chronological order!**\n",
    "\n",
    "As long as the filenames include a sensible timestamp, such as a date in `YYYYMMDD` (Year-Month-Day) order, we can use call the `sort()` method of the Python list to get the files in alphanumeric order, which is the same as chronological order in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e44bd-acb6-4622-ba67-97861f278ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list.sort()\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d60b3-b781-4821-9754-13981dcba982",
   "metadata": {},
   "source": [
    "Let's write a `for` loop to process each SMAP L3 granule and extract the mean within this rectangular window (indicated by the upper-left and lower-right corners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74b5c0-d902-497a-8207-a7dcd69aa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_left = ease2_from_wgs84((-109, 49), grid = 'M36')\n",
    "lower_right = ease2_from_wgs84((-97, 43), grid = 'M36')\n",
    "r0, c0 = upper_left\n",
    "r1, c1 = lower_right\n",
    "\n",
    "sm_mean = []\n",
    "for filename in file_list:\n",
    "    ds = process_smap_l3(filename)\n",
    "    sm_mean.append(ds['soil_moisture_dca_pm'][r0:r1,c0:c1].mean().values)\n",
    "\n",
    "sm_mean = np.hstack(sm_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f3aa3-8f7e-44c6-925a-d7f9c98cc973",
   "metadata": {},
   "source": [
    "When we plot the data, it would be nice to show dates along the horizontal axis. We can get a sequence of dates using the `pandas.date_range()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fe3b4-5344-450e-a86a-98b9006a2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "dates = pandas.date_range('2017-06-01', '2017-09-30', freq = '1D')\n",
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.plot(dates, sm_mean, 'k-')\n",
    "pyplot.ylabel('Volumetric Soil Moisture (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a7c3d-906e-4508-9669-af4f7bc16ea8",
   "metadata": {},
   "source": [
    "**Our time series looks strange.** There are several two-day gaps (where the line is broken) and a lot of high-frequency variation (spikes). These spikes seem to occur just before or after the gaps. We can intuit that the gaps correspond to days where the SMAP satellite did not pass over our study area. We also know there are days when our study area is only partially observed (as we saw in the plot above) and these likely correspond to the extreme values, as wetter or drier parts of the study area are missed.\n",
    "\n",
    "It's clear that we should not be calculating a mean value when only part of the study area is observed, as this is causing bias (and the spikes, above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e6f4e-eff7-42e4-8578-ea8363a2d525",
   "metadata": {},
   "source": [
    "### Calculating a moving average\n",
    "\n",
    "One way to address the gaps might be to calculate a moving average, filling in missing values for a given date with the average of the values from adjacent dates.\n",
    "\n",
    "Below, we use two nested `for` loops to create a composite soil moisture map for each date by combining the images from the current, previous, and next days (i.e., a 3-day moving average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44023e3-5698-4dc0-8fd5-ce40f260a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = []\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    # Skip the first and last files\n",
    "    if i == 0 or i == (len(file_list) - 1):\n",
    "        continue\n",
    "\n",
    "    # For the previous, current, and next dates...\n",
    "    sm_stack = []\n",
    "    for j in [i-1, i, i+1]:\n",
    "        ds = process_smap_l3(file_list[j])\n",
    "        sm = ds['soil_moisture_dca_pm'][r0:r1,c0:c1]\n",
    "        sm_stack.append(sm)\n",
    "\n",
    "    # Take the average of the 3 values in each pixel, excluding NaNs\n",
    "    sm_stack = np.nanmean(np.stack(sm_stack, axis = 0), axis = 0)\n",
    "    # Then, compute the overall mean for the region of interest\n",
    "    time_series.append(sm_stack.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e2f6a-dd80-414e-93bc-a2a558fd9a4f",
   "metadata": {},
   "source": [
    "We still have 120 days of data, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58063d-2bb6-4d33-a1e2-347bcc4ed1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970cd94-8c76-4ed7-9133-08dc911a3738",
   "metadata": {},
   "source": [
    "And each date is now a composite of three daily images. We can take a look at the last one that was processed, above, by plotting the `sm_stack` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31715b5c-3767-4a95-ae6e-b83a3ff475ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(sm_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd0bf9-a447-4347-9fa5-ae52351b312e",
   "metadata": {},
   "source": [
    "We now obtain a soil moisture time series that looks a little more reasonable. \n",
    "\n",
    "It's apparent from our time series that 2017 was a fairly dry summer overall but that soil moisture in the region reached a minimum during the flash drought, between `2017-09-01` and `2017-09-15`. Soil moisture also increases by a large amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea374a-abb7-42ba-bd14-5b181c95c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.plot(dates[1:-1], time_series, 'k-')\n",
    "pyplot.ylabel('Volumetric Soil Moisture (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64bda8-8d1d-4ef1-87a9-5fed52fd8a56",
   "metadata": {},
   "source": [
    "### Summary: Reading HDF5 and netCDF4 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b387aa-ec14-4496-a012-8bceff9b602c",
   "metadata": {},
   "source": [
    "|                            |  HDF5 files                        | netCDF4 files                          | `xarray` (for both)        |\n",
    "|:---------------------------|:-----------------------------------|:---------------------------------------|:---------------------------|\n",
    "|Module import               | `import h5py`                      | `import netCDF4`                       | `import xarray as xr`      |\n",
    "|Files opened with...        | `hdf = h5py.File(...)`             | `nc = netCDF4.Dataset()`               | `ds = xr.open_dataset()`   |\n",
    "|Datasets/groups viewed...   | `hdf.keys()`                       | `nc.variables` or `nc.variables.keys()`| `list(ds.variables.keys())`|\n",
    "|                            | `hdf['group_name'].keys()`         | `nc.variables['group_name'].keys()`    |                            |\n",
    "|Datasets accessed through...| `hdf`                              | `nc.variables`                         | `ds.variables`             |\n",
    "|Attributes listed through...| `hdf.attrs`                        | `nc.ncattrs()`                         | `ds.attrs`                 |\n",
    "|                            | `hdf['dataset'].attrs`             | `nc.variables['dataset'].ncattrs()`    |                            |\n",
    "|Attributes read by...       | `hdf['dataset'].attrs['attribute']`| `nc.variables['dataset'].getncattr()`  | `ds.variables['dataset']`  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f04ad-61cd-40c5-8db1-a218161b48ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## More resources\n",
    "\n",
    "- Curious about how to use `earthaccess.open()` along with `xarray` so that you don't have keep any downloaded files around? Well, `xarray.open_dataset()` can be slow when you have a lot of files to open, as in this time-series example. [This article describes how you can speed up `xarray.open_dataset()`](https://climate-cms.org/posts/2018-09-14-dask-era-interim.html) when working with multiple cloud-hosted files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ae193-7a05-4bde-8264-209483e74743",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Chen, L. G., J. Gottschalck, A. Hartman, D. Miskus, R. Tinker, and A. Artusa. 2019. Flash drought characteristics based on U.S. Drought Monitor. Atmosphere 10 (9):498.\n",
    "- He, M., J. S. Kimball, Y. Yi, S. W. Running, K. Guan, K. Jensco, B. Maxwell, and M. Maneta. 2019. Impacts of the 2017 flash drought in the US Northern plains informed by satellite-based evapotranspiration and solar-induced fluorescence. Environmental Research Letters 14 (7):074019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
