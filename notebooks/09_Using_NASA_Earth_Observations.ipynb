{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79bde19-ec9b-401e-8ad0-6bfad1ad6dc3",
   "metadata": {},
   "source": [
    "# M1.9 - Using NASA Earth Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24afa7-54b1-45bc-ab6b-9164cab19231",
   "metadata": {},
   "source": [
    "*Part of:* **M1: Open Climate Data**\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "1. [Organizing our file system](#Organizing-our-file-system)\n",
    "1. [Using NASA climate observations](#Using-NASA-climate-observations)\n",
    "   - [Downloading SMAP Level 3 soil moisture data](#Downloading-SMAP-Level-3-soil-moisture-data)\n",
    "   - [Customizing an Earthdata Search download](#Customizing-an-Earthdata-Search-download)\n",
    "1. [Understanding hierarchical data files (HDF5)](#Understanding-hierarchical-data-files-(HDF5))\n",
    "   - [Reading HDF5 datasets](#Reading-HDF5-datasets)\n",
    "   - [Subsetting the SMAP L3 data](#Subsetting-the-SMAP-L3-data)\n",
    "1. [Creating a soil moisture time series](#Creating-a-soil-moisture-time-series)\n",
    "   - [Calculating a moving average](#Calculating-a-moving-average)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c13a88-9869-4dad-8384-e863e384589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7b524-c0a6-46d7-9a8c-255d2a9a5c55",
   "metadata": {},
   "source": [
    "## Organizing our file system\n",
    "\n",
    "Again, we'll need a place to store these raw data. When we started working with Noah NLDAS data, we created the following folders in our file system:\n",
    "\n",
    "```\n",
    "data_raw/\n",
    "  NLDAS\n",
    "  SMAP_L3\n",
    "```\n",
    "\n",
    "Make sure there is also a `SMAP_L3` folder to receive the data we're about to download!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762ecbd-552c-4892-a296-2292906776a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using NASA climate observations\n",
    "\n",
    "The NLDAS data we've used are a great tool for retrospective studies but, as a re-analysis dataset, it has some limitations:\n",
    "\n",
    "- It has a relatively high latency; it may be days or weeks before data are available.\n",
    "- It integrates data from multiple sources but with varying levels of accuracy and geographic coverage.\n",
    "\n",
    "If we want to characterize flash drought or detect it in near-real time, we shouldn't use re-analysis datasets. Instead, we want some kind of direct observation of drought conditions. **Let's see what we can learn about the 2017 Flash Drought from NASA's satellite-based soil-moisture estimates.**\n",
    "\n",
    "**We'll use data from NASA's Soil Moisture Active Passive (SMAP) Mission.** [NASA's earth observing missions provide data that is grouped into different processing levels:](https://www.earthdata.nasa.gov/engage/open-data-services-and-software/data-information-policy/data-levels)\n",
    "\n",
    "- **Level 1 (Raw data):** Basically, these are data values measured directly by a satellite instrument. They may or may not be physically interpretable. Most end-users won't benefit from Level 1 data.\n",
    "- **Level 2:** These are physically interpretable values that have been derived from the raw data, at the same spatial and temporal resolution as the Level 1 data. Level 2 data may be hard to use because the spatial structure of the data matches the instrument's viewing geometry.\n",
    "- **Level 3:** At Level 3, the geophysical values have been standardized on a uniform spatial grid and uniform time series. While some values may be missing due to low quality, clouds, or sensor failure, gridded Level 3 data from different time steps can be easily combined and compared.\n",
    "- **Level 4 (Model-enhanced data):** At Level 4, the values from Level 3 data are incorporated into some kind of model, possibly combining additional, independent datasets from other sensors in order to produce enhanced estimates or analyses of geophysical variables.\n",
    "\n",
    "### Downloading SMAP Level 3 soil moisture data\n",
    "\n",
    "**[We'll use the 36-km Level 3 surface soil moisture data from the SMAP mission](https://nsidc.org/data/spl3smp/versions/8)** because these are a good compromise between direct sensor observations and ease of use.\n",
    "\n",
    "- At the website above, we can see there are multiple ways of accessing the data. [Let's use Earthdata Search;](https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008) can we access the data from NASA's cloud using `earthaccess`?\n",
    "- You may have noticed that the Level 3 SMAP data we want to use are *not* \"Available in Earthdata Cloud.\" It looks like we'll have to download the data directly.\n",
    "- **Where will we put the raw data we download?** Let's revisit our file tree in Jupyter Notebook.\n",
    "- **Within the `data_raw` folder, let's create a new folder called `SMAP_L3`.** This is where we'll put the data we're about to download.\n",
    "\n",
    "We've discussed the importance of having a well-documented workflow that makes it easy to understand how we obtained a particular scientific result. We assume that we can re-download the raw data we used anytime, but what if we forget where the data came from? Since the SMAP Level 3 data aren't available in the Cloud, we're about to do download the data manually, and it would be a good idea to document what steps we took to do that, in case there are questions about where the data came from or what kind of processing was applied.\n",
    "\n",
    "- In the Jupyter Notebook file tree, within the `SMAP_L3` folder let's make a new `\"New File\"`. Name the new text file `README.txt`.\n",
    "- Double-click `README.txt` to open it. This is where we'll add some useful information about the data we're about to download. Below is an example.\n",
    "\n",
    "```\n",
    "Author: K. Arthur Endsley\n",
    "Date: November 1, 2023\n",
    "\n",
    "This folder contains Level 3 data from the SMAP Mission. It was downloaded from:\n",
    "\n",
    "    https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008\n",
    "\n",
    "Here's some more information about this product:\n",
    "\n",
    "    https://nsidc.org/data/spl3smp/versions/8\n",
    "```\n",
    "\n",
    "This might not seem like a lot of information but there's plenty here that we would want to know if we took a long break from this project or if someone else had to try and figure out what we were doing. And its short length is also an advantage: **documenting your project doesn't have to be hard and any amount of information is better than none.**\n",
    "\n",
    "### Customizing an Earthdata Search download\n",
    "\n",
    "The SMAP satellite has two overpasses every day, a \"morning\" and an \"afternoon\" overpass (local time). Let's use soil moisture data from the afternoon (PM) overpass, because this is likely when soil moisture stress on vegetation is at its peak.\n",
    "\n",
    "- [**This link will get you to the right place to start.**](https://search.earthdata.nasa.gov/search?q=SPL3SMP%20V008) Click on the one dataset that is shown on the right-hand side of the search window.\n",
    "- We'll download data from August and September to study the onset and progression of the 2017 Flash Drought: **Choose a temporal subset, 2017-06-01 through 2017-09-30.**\n",
    "- At the bottom right, **click the big green button that reads \"Download All.\"**\n",
    "- 3.8 GB is a lot of data! Can we make this download any smaller? We're only interested in soil moisture from the afternoon overpass. **Click \"Edit Options\" and under \"Select a data access method,\" select the \"Customize\" option.**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order.png)\n",
    "\n",
    "- **Scroll down to \"Configure data customization options\" and down to \"Band subsetting.\"**\n",
    "- **Within the text box that reads \"Filter\" type `soil_moisture_dca_pm`.** This will filter the available variables (\"bands\") to just this specific variable, which is the soil moisture estimate from the Dual-Channel Algorithm (DCA) for the afternoon (PM) overpass.\n",
    "- To make sure we're only downloading the fields we want, **you'll need to uncheck the box next to `SPL3SMP` then re-check the box next to `soil_moisture_dca_pm` (see screenshot below) and each variable we want to keep.**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order_variables.png)\n",
    "\n",
    "**We want to download the following fields (only):**\n",
    "\n",
    "- `soil_moisture_dca_pm`\n",
    "- `static_water_body_fraction_pm`\n",
    "- `retrieval_qual_flag_dca_pm`\n",
    "  \n",
    "Hit \"Done\" at the bottom of this form then the big green button that reads \"Download Data\"!\n",
    "\n",
    "#### But Wait!\n",
    "\n",
    "Because we selected a subset of variables, we'll have to wait to get an e-mail that the order is ready. **You don't need to do these steps yourself, because I already prepared all the data granules that would be downloaded this way.** They can be download directly from here:\n",
    "\n",
    "- [SMAP_L3_SPL3SMP_V008_20170601_20170930.zip](http://files.ntsg.umt.edu/data/ScienceCore/SMAP_L3_SPL3SMP_V008_20170601_20170930.zip) (Extract this ZIP file's contents to your `data_raw/SMAP_L3` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fcfd0-56f9-4ad7-9998-4104e4680432",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Understanding hierarchical data files (HDF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae66ba-1f3f-4ead-a899-b68503ae252d",
   "metadata": {},
   "source": [
    "The SMAP Level 3 data we downloaded are each stored as a **Hierarchical Data File, version 5 (HDF5).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af777f84-05bf-4a48-b6d7-11ee827faf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = 'data_raw/SMAP_L3/SMAP_L3_SM_P_20170901_R18290_001.h5'\n",
    "hdf = h5py.File(filename, 'r')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc21c83-44dc-4ef4-a7a6-dad557c0241c",
   "metadata": {},
   "source": [
    "An HDF5 file is a lot like a netCDF4 file: they are both hierarhical files capable of storing multiple, diverse datasets and metadata in a single file. What do we mean by \"hierarchical\"? Well, an HDF5 or netCDF4 file is like a file tree, where *datasets* can be organized into different nested *groups,* as depicted below. Metadata, in the form of *attributes,* can be attached to any dataset or group throughout the file.\n",
    "\n",
    "![](assets/hdf5-structure.jpg)\n",
    "\n",
    "*Image courtesy of NEON Science.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b05df1-cebb-4fb9-b532-c41d542047c2",
   "metadata": {},
   "source": [
    "We can look at the groups and datasets that are at the highest level of this hierarchy by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f478567-664d-48cf-990a-f38ad78fa1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67bfe3-965a-41fd-9e0f-c3a798f759c2",
   "metadata": {},
   "source": [
    "The `h5py.File` object, `hdf`, is accessed like a Python dictionary. If we want to look at the `'Metadata'` group, for example, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73580d-bc31-4f59-8064-ccce0d0ef783",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682e275-cd4b-49b2-aecf-61712aacec7a",
   "metadata": {},
   "source": [
    "This isn't very informative, but every group and dataset in an `h5py.File` object also behaves like a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9469661-4435-4472-b429-ccee11945eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eedb81-b0af-4ecf-8cee-46a70ca60fcd",
   "metadata": {},
   "source": [
    "The `'Metadata'` group is an example of how we might store information in an HDF5 file other than multi-dimensional arrays.\n",
    "\n",
    "What is the significance of this empty group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cad114-a67c-45d4-8941-e5914205bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a939d-338a-4dbe-a761-cf7976d4d308",
   "metadata": {},
   "source": [
    "Just like netCDF files, every dataset in an HDF5 file can be labeled with attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc84ca9-828b-411c-ad3b-4330e30fa509",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66735587-3c8e-4572-a004-338c00222ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs['processor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573327d-a61f-45fe-8b82-9c2598767731",
   "metadata": {},
   "source": [
    "### Reading HDF5 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfa9d8-3973-4632-9b86-8e5471e02dfe",
   "metadata": {},
   "source": [
    "We open an HDF5 file for reading with the `'r'` flag, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23df3c-e8c9-4f72-8f6d-09b53342d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = h5py.File(filename, 'r')\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edc4fa-fbee-4f2a-89f7-8e7e31bf4144",
   "metadata": {},
   "source": [
    "Again, we can access datasets hierarchically..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44bc8d-9e70-4358-bdb8-3ffcc101ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Soil_Moisture_Retrieval_Data_PM'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5406d-66b4-4860-8b6c-200f1b8752f2",
   "metadata": {},
   "source": [
    "And if we want to read a dataset, we use NumPy's `[:]` notation to indicate we want to access an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20daf3-c174-460a-90c1-16d6d625fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Soil_Moisture_Retrieval_Data_PM/soil_moisture_dca_pm'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed4753-4aef-4a43-9c3e-0a847cbdc8e4",
   "metadata": {},
   "source": [
    "Whenever we're finished working with an open HDF5 file, we should make sure to close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281dc17-0562-4902-b878-ba86687911fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cf24a-8164-49e9-aac7-d155880a3bcc",
   "metadata": {},
   "source": [
    "**Let's see what is different about opening the same file using `xarray`.** In particular, look at the **Data variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd13e73-0f04-4ab1-9a15-ad5dcc0fa21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filename)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81ced6-0b65-48b4-8290-04fada92528c",
   "metadata": {},
   "source": [
    "The single variable that was found, `\"crs\"`, is not going to be very useful to us.\n",
    "\n",
    "**`xarray` has limitations when opening HDF5 files; it isn't able to determine what groups are available.** Instead, we have to specify the group we want to open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4937b54-50e1-43de-b426-23745f86caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filename, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b6adc-f6ab-4ee8-8ac6-5503f7b9ec6b",
   "metadata": {},
   "source": [
    "Now we have a useful variable, `\"soil_moisture_dca_pm\"`, but our `xarray.Dataset` has no coordinates!\n",
    "\n",
    "One way to fix this would be to assign coordinates to our `xarray.Dataset`. This is why we need the `h5py` library, which is specialized for handling HDF5 files. We can read the `\"x\"` and `\"y\"` coordinates from our `h5py.File` and write them to the `xarray.Dataset`, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf531a-2dab-4067-8db3-5bdde91b4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor: Note coordinates assignment\n",
    "\n",
    "hdf = h5py.File(filename, 'r')\n",
    "ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
    "hdf.close()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad3ae9-9d89-4266-8c56-dcb28bee3054",
   "metadata": {},
   "source": [
    "Now that we have both a **data variable** and **coordinates,** we're ready to plot the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa77a5-e5e3-4a5d-95ba-8c6fbd7a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (12, 5))\n",
    "ds['soil_moisture_dca_pm'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410de18-9d8f-40df-98ec-e8ac2452c6d9",
   "metadata": {},
   "source": [
    "There are two things to note about this image:\n",
    "\n",
    "- **It's right-side up!** Unlike netCDF4 files, HDF5 files don't enforce a convention regarding the direction of spatial coordinates.\n",
    "- **Notice the striping in this image.** The SMAP satellite has a revisit time of between 2 and 3 days. This means that, on a single day, the satellite's radiometer only images part of the globe. We could combine the morning, `\"soil_moisture_dca_am\"`, and afternoon, `\"soil_moisture_dca_pm\"`, overpasses for a single day, but soil moisture in many regions of the world varies quite a lot between morning and afternoon, so this might not be reasonable.\n",
    "\n",
    "We chose the `\"soil_moisture_dca_pm\"` (afternoon) overpass because the afternoon is typically when soil moisture stress is highest in terrestrial ecosystems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb921e8-3f9e-48a1-9539-227b5a0e616e",
   "metadata": {},
   "source": [
    "### Challenge: Write a function to process SMAP L3 data\n",
    "\n",
    "Based on what we just did above, write a single function called `process_smap_l3` that:\n",
    "\n",
    "- Accepts a file path to a SMAP L3 `*.h5` file, as a Python string\n",
    "- Returns an `xr.Dataset`\n",
    "\n",
    "When you've finished, compare it to the one written below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620a0f9-6c71-4d88-add7-44a86e73b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smap_l3(file_path):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The file path to the SMAP L3 file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "    '''\n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "        return ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee212fd6-14f6-429c-b4d5-7893179250e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Subsetting the SMAP L3 data\n",
    "\n",
    "The SMAP soil moisture data are global but we're currently interested in a small study region, the Northern Plains of the U.S. How can we subset the SMAP data to our study region?\n",
    "\n",
    "You may have noticed that the coordinates we added to our `xarray.Dataset`, above, were not latitude-longitude coordinates. The SMAP data are projected onto an EASE-Grid 2.0, where \"EASE\" stands for Equal-Area Scalable Earth. This unique, global projection has many advantages but the X and Y coordinates can be hard to understand when we're used to working with latitude-longitude coordinates.\n",
    "\n",
    "**We'll use a tool from the `pyl4c` library to translate latitude-longitude (WGS84 datum) coordinates into the row-column coordinates of pixels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097031-2c99-4c26-a433-2bd7ef4a6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyl4c.ease2 import ease2_from_wgs84\n",
    "\n",
    "help(ease2_from_wgs84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc96f93-675b-4053-8b22-9c40a05c5356",
   "metadata": {},
   "source": [
    "Let's say that the upper-left corner of our study area is 49 degrees N latitude, 109 degrees W longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8b08d-3a89-4161-bd3f-687d5f6010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the upper-left corner coordinates\n",
    "upper_left = ease2_from_wgs84((-109, 49), grid = 'M36')\n",
    "upper_left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c4462-b57c-4114-a77c-0789c7f4a8c0",
   "metadata": {},
   "source": [
    "And the lower-right corner of our study area is 43 degrees N latitude, 95 degrees W longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c193cd3-a6c5-4438-8093-c1cb3658b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_right = ease2_from_wgs84((-95, 43), grid = 'M36')\n",
    "lower_right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68416bb9-4599-42bf-910f-51b369d4af72",
   "metadata": {},
   "source": [
    "Let's get an `xarray.Dataset` using the function we wrote and plot our study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780896-f79e-493c-bbfa-99c049824b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_smap_l3('data_raw/SMAP_L3/SMAP_L3_SM_P_20170802_R18290_001.h5')\n",
    "\n",
    "ds['soil_moisture_dca_pm'][49:59,187:227].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f9dd8-0a75-493e-ad70-08423c41c152",
   "metadata": {},
   "source": [
    "There are two things to note from this image:\n",
    "\n",
    "- **It's apparent that the western part of our study area was missed by the satellite on this particular data-day.**\n",
    "- **There is an area with very high soil moisture (bright yellow pixel) in the left-center of the image.** This pixel is almost certainly a permanent water body, so we should mask it out before doing any analysis.\n",
    "\n",
    "Despite the missing area and water body, it's still possible to get a mean soil moisture value for the region..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6c111-b71c-4cde-b474-3f72d528138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['soil_moisture_dca_pm'][49:59,187:227].mean().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce3442-30a2-4f85-8239-d5d6d0f29d19",
   "metadata": {},
   "source": [
    "But this approach may be biased because of the water body and because, depending on the day, the value reflects the soil moisture conditions in different, smaller parts of our study region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424a44b-2747-4bc7-9dba-b25f3d470240",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Masking out permanent water bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5312a-ad29-4f7f-bf64-7f0aa2591179",
   "metadata": {},
   "source": [
    "Let's take a look at the `static_water_body_fraction_pm` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a8471-1170-47b4-bea7-09dbddaa767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (12, 5))\n",
    "ds['static_water_body_fraction_pm'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683453a8-2543-4fb1-af6b-dc39e7b72b5c",
   "metadata": {},
   "source": [
    "Although the water body fraction is \"static\" and doesn't change over time, the SMAP L3 dataset only shows that part of the mask where data were acquired, hence the striping.\n",
    "\n",
    "And let's focus on our study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093541f-8ad6-4322-b473-d88bed88278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['static_water_body_fraction_pm'][49:59,187:227].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c8783-deba-46e0-aa81-08761f58039b",
   "metadata": {},
   "source": [
    "How could we use this to mask out permanent water bodies in our soil moisture data? We'll need to decide what threshold for fractional water coverage will be used to define permanent water bodies. Let's adopt the convention that pixels with greater than 20% of their area covered by water (i.e., fractional water area $\\ge 0.2$) should be masked.\n",
    "\n",
    "The above image reminds us of another issue we need to address: each daily image may include only part of our study area. **In each daily granule, we can use the granule's `static_water_body_fraction_pm` dataset to create a binary mask where `1` indicates a permanent water body and `0` indicates anything else (valid data).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf9779-38bf-4216-bc46-9743f2185cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(ds['static_water_body_fraction_pm'][:] >= 0.2, 1, 0)[49:59,187:227]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ef595-e11f-48f0-a327-a69d5c34e0b7",
   "metadata": {},
   "source": [
    "Let's update our `process_smap_l3()` function to include masking of the data. **Note that we've added a keyword argument to our function to allow users to have control over the fractional water area threshold used. By providing a default argument to `threshold`, we are also implicitly documenting our decision to use `0.2` as the threshold.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98502d44-682f-4dd3-b44d-b7a4b0a749fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smap_l3(file_path, threshold = 0.2):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The file path to the SMAP L3 file\n",
    "    threshold : float\n",
    "        The fractional water area threshold to use when masking out\n",
    "        permnanet water bodies\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "    '''\n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "        ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
    "\n",
    "    # Create a binary (0 or 1) array based on the water fraction threshold\n",
    "    mask = np.where(ds['static_water_body_fraction_pm'][:] >= threshold, 1, 0)\n",
    "    \n",
    "    # Read the dataset out as a NumPy array so we can mask it\n",
    "    data = ds['soil_moisture_dca_pm'].to_numpy()\n",
    "    \n",
    "    # Write NaN into this dataset wherever there are permanent water bodies\n",
    "    data[mask == 1] = np.nan\n",
    "    ds['soil_moisture_dca_pm'][:] = data\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def7a50-3b54-4ac6-9a8d-61837cc64ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_smap_l3('data_raw/SMAP_L3/SMAP_L3_SM_P_20170802_R18290_001.h5')\n",
    "ds['soil_moisture_dca_pm'][49:59,187:227].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1dc89-86d7-4aff-bc24-9f24b35b2202",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating a soil moisture time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e907b6e-9fdd-4099-9098-be798bd53e34",
   "metadata": {},
   "source": [
    "It'd be nice if we could use `xr.open_mf_dataset()` to open all these SMAP HDF5 files as a single time-series dataset. If we tried that, however, we'd find that it doesn't work because `xarray` doesn't know what the coordinates of an HDF5 dataset are, so it can't combine the datasets together.\n",
    "\n",
    "```python\n",
    "ds = xr.open_mfdataset('data_raw/SMAP_L3/*.h5', group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "```\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[166], line 1\n",
    "----> 1 ds = xr.open_mfdataset('data_raw/SMAP_L3/*.h5', group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "\n",
    "...\n",
    "\n",
    "ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8915f6-1620-42f8-ac12-0073d8c722bf",
   "metadata": {},
   "source": [
    "**This means we'll have to open each file ourselves and stack the arrays together.**\n",
    "\n",
    "We can use the `glob` library to get a list of all the files we want. The notation below, `'data_raw/SMAP_L3/*.h5'`, is similar to what we used with `xr.open_mfdataset()`: we're saying we want to use all the HDF5 files in a particular directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83708eb4-2559-42a4-889a-55d34041ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('data_raw/SMAP_L3/*.h5')\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909bd02-cb62-4509-9dfa-d3d099d5d8bd",
   "metadata": {},
   "source": [
    "**When we use `glob.glob()` for files that represent a time series, it's very important that we make sure the files are listed in chronological order!**\n",
    "\n",
    "As long as the filenames include a sensible timestamp, such as a date in `YYYYMMDD` (Year-Month-Day) order, we can use call the `sort()` method of the Python list to get the files in alphanumeric order, which is the same as chronological order in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e44bd-acb6-4622-ba67-97861f278ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list.sort()\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d60b3-b781-4821-9754-13981dcba982",
   "metadata": {},
   "source": [
    "Let's write a `for` loop to process each SMAP L3 granule and extract the mean within this rectangular window (indicated by the upper-left and lower-right corners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74b5c0-d902-497a-8207-a7dcd69aa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_left = ease2_from_wgs84((-109, 49), grid = 'M36')\n",
    "lower_right = ease2_from_wgs84((-97, 43), grid = 'M36')\n",
    "r0, c0 = upper_left\n",
    "r1, c1 = lower_right\n",
    "\n",
    "sm_mean = []\n",
    "for filename in file_list:\n",
    "    ds = process_smap_l3(filename, threshold = 0.2)\n",
    "    sm_mean.append(ds['soil_moisture_dca_pm'][r0:r1,c0:c1].mean().values)\n",
    "\n",
    "sm_mean = np.hstack(sm_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f3aa3-8f7e-44c6-925a-d7f9c98cc973",
   "metadata": {},
   "source": [
    "When we plot the data, it would be nice to show dates along the horizontal axis. We can get a sequence of dates using the `pandas.date_range()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fe3b4-5344-450e-a86a-98b9006a2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "dates = pandas.date_range('2017-06-01', '2017-09-30', freq = '1D')\n",
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.plot(dates, sm_mean, 'k-')\n",
    "pyplot.ylabel('Volumetric Soil Moisture (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a7c3d-906e-4508-9669-af4f7bc16ea8",
   "metadata": {},
   "source": [
    "**Our time series looks strange.** There are several two-day gaps (where the line is broken) and a lot of high-frequency variation (spikes). These spikes seem to occur just before or after the gaps. \n",
    "\n",
    "**Are these spikes due to permanent water bodies we failed to mask out? One advantage of having a `threshold` parameter in our `process_smap_l3()` function is that we can test this theory pretty quickly: change the `threshold` to `0.1` and see for yourself!**\n",
    "\n",
    "Unfortunately, that doesn't seem to be the case. We can intuit that the gaps correspond to days where the SMAP satellite did not pass over our study area. We also know there are days when our study area is only partially observed (as we saw in the plot above) and these likely correspond to the extreme values, as wetter or drier parts of the study area are missed.\n",
    "\n",
    "It's clear that we should not be calculating a mean value when only part of the study area is observed, as this is causing bias (and the spikes, above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e6f4e-eff7-42e4-8578-ea8363a2d525",
   "metadata": {},
   "source": [
    "### Calculating a moving average\n",
    "\n",
    "One way to address the gaps might be to calculate a moving average, filling in missing values for a given date with the average of the values from adjacent dates.\n",
    "\n",
    "Below, we use two nested `for` loops to create a composite soil moisture map for each date by combining the images from the current, previous, and next days (i.e., a 3-day moving average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44023e3-5698-4dc0-8fd5-ce40f260a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = []\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    # Skip the first and last files\n",
    "    if i == 0 or i == (len(file_list) - 1):\n",
    "        continue\n",
    "\n",
    "    # For the previous, current, and next dates...\n",
    "    sm_stack = []\n",
    "    for j in [i-1, i, i+1]:\n",
    "        ds = process_smap_l3(file_list[j])\n",
    "        sm = ds['soil_moisture_dca_pm'][r0:r1,c0:c1]\n",
    "        sm_stack.append(sm)\n",
    "\n",
    "    # Take the average of the 3 values in each pixel, excluding NaNs\n",
    "    sm_stack = np.nanmean(np.stack(sm_stack, axis = 0), axis = 0)\n",
    "    # Then, compute the overall mean for the region of interest\n",
    "    time_series.append(sm_stack.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e2f6a-dd80-414e-93bc-a2a558fd9a4f",
   "metadata": {},
   "source": [
    "We still have 120 days of data, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58063d-2bb6-4d33-a1e2-347bcc4ed1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970cd94-8c76-4ed7-9133-08dc911a3738",
   "metadata": {},
   "source": [
    "And each date is now a composite of three daily images. We can take a look at the last one that was processed, above, by plotting the `sm_stack` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31715b5c-3767-4a95-ae6e-b83a3ff475ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(sm_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd0bf9-a447-4347-9fa5-ae52351b312e",
   "metadata": {},
   "source": [
    "We now obtain a soil moisture time series that looks a little more reasonable. \n",
    "\n",
    "It's apparent from our time series that 2017 was a fairly dry summer overall but that soil moisture in the region reached a minimum during the flash drought, between `2017-09-01` and `2017-09-15`. Soil moisture also increases by a large amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea374a-abb7-42ba-bd14-5b181c95c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.plot(dates[1:-1], time_series, 'k-')\n",
    "pyplot.ylabel('Volumetric Soil Moisture (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64bda8-8d1d-4ef1-87a9-5fed52fd8a56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- NASA Earthdata Search provides many different datasets for studying earth's climate system. Re-analysis datasets and NASA Level 4 datasets integrate multiple raw data sources to provide a continuous record and complete spatial coverage, without data gaps. However, they may include model biases that are not reflective of real-world conditions. Remote sensing datasets and NASA Level 3 datasets offer more direct observations.\n",
    "\n",
    "- **Document your data!** When you download raw data, keep it separate and unmodified. Be sure to make a `README` file, placed in the same directory or the parent directory, that contains information on where the data came from and how it should be used.\n",
    "\n",
    "- **Document your process!** When working with data, re-useable Python functions can help speed up your work. Well-written functions also serve as a documentation of your workflow, as they describe the steps you took to process data.\n",
    "\n",
    "- **HDF5 and netCDF4 files can help you stay organized.** Both file formats allow you to add *attributes* to datasets they contain, which is a good way to document measurement units, original data sources, and other key *metadata.*\n",
    "\n",
    "### Reading HDF5 and netCDF4 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b387aa-ec14-4496-a012-8bceff9b602c",
   "metadata": {},
   "source": [
    "|                            |  HDF5 files                        | netCDF4 files                          | `xarray` (for both)        |\n",
    "|:---------------------------|:-----------------------------------|:---------------------------------------|:---------------------------|\n",
    "|Module import               | `import h5py`                      | `import netCDF4`                       | `import xarray as xr`      |\n",
    "|Files opened with...        | `hdf = h5py.File(...)`             | `nc = netCDF4.Dataset()`               | `ds = xr.open_dataset()`   |\n",
    "|Datasets/groups viewed...   | `hdf.keys()`                       | `nc.variables` or `nc.variables.keys()`| `list(ds.variables.keys())`|\n",
    "|                            | `hdf['group_name'].keys()`         | `nc.variables['group_name'].keys()`    |                            |\n",
    "|Datasets accessed through...| `hdf`                              | `nc.variables`                         | `ds.variables`             |\n",
    "|Attributes listed through...| `hdf.attrs`                        | `nc.ncattrs()`                         | `ds.attrs`                 |\n",
    "|                            | `hdf['dataset'].attrs`             | `nc.variables['dataset'].ncattrs()`    |                            |\n",
    "|Attributes read by...       | `hdf['dataset'].attrs['attribute']`| `nc.variables['dataset'].getncattr()`  | `ds.variables['dataset']`  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f04ad-61cd-40c5-8db1-a218161b48ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## More resources\n",
    "\n",
    "- Curious about how to use `earthaccess.open()` along with `xarray` so that you don't have keep any downloaded files around? Well, `xarray.open_dataset()` can be slow when you have a lot of files to open, as in this time-series example. [This article describes how you can speed up `xarray.open_dataset()`](https://climate-cms.org/posts/2018-09-14-dask-era-interim.html) when working with multiple cloud-hosted files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ae193-7a05-4bde-8264-209483e74743",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Chen, L. G., J. Gottschalck, A. Hartman, D. Miskus, R. Tinker, and A. Artusa. 2019. Flash drought characteristics based on U.S. Drought Monitor. Atmosphere 10 (9):498.\n",
    "- He, M., J. S. Kimball, Y. Yi, S. W. Running, K. Guan, K. Jensco, B. Maxwell, and M. Maneta. 2019. Impacts of the 2017 flash drought in the US Northern plains informed by satellite-based evapotranspiration and solar-induced fluorescence. Environmental Research Letters 14 (7):074019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
